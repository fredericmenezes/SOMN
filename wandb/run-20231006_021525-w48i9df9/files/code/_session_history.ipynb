{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c48589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f733ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "# tentando login\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1f7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ec1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common import type_aliases\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, VecMonitor, is_vecenv_wrapped\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "\n",
    "def evaluate_policy(\n",
    "    model: \"type_aliases.PolicyPredictor\",\n",
    "    env: Union[gym.Env, VecEnv],\n",
    "    n_eval_episodes: int = 10,\n",
    "    deterministic: bool = True,\n",
    "    render: bool = False,\n",
    "    callback: Optional[Callable[[Dict[str, Any], Dict[str, Any]], None]] = None,\n",
    "    reward_threshold: Optional[float] = None,\n",
    "    return_episode_rewards: bool = False,\n",
    "    warn: bool = True,\n",
    ") -> Union[Tuple[float, float], Tuple[List[float], List[int]]]:\n",
    "    \"\"\"\n",
    "    Runs policy for ``n_eval_episodes`` episodes and returns average reward.\n",
    "    If a vector env is passed in, this divides the episodes to evaluate onto the\n",
    "    different elements of the vector env. This static division of work is done to\n",
    "    remove bias. See https://github.com/DLR-RM/stable-baselines3/issues/402 for more\n",
    "    details and discussion.\n",
    "\n",
    "    .. note::\n",
    "        If environment has not been wrapped with ``Monitor`` wrapper, reward and\n",
    "        episode lengths are counted as it appears with ``env.step`` calls. If\n",
    "        the environment contains wrappers that modify rewards or episode lengths\n",
    "        (e.g. reward scaling, early episode reset), these will affect the evaluation\n",
    "        results as well. You can avoid this by wrapping environment with ``Monitor``\n",
    "        wrapper before anything else.\n",
    "\n",
    "    :param model: The RL agent you want to evaluate. This can be any object\n",
    "        that implements a `predict` method, such as an RL algorithm (``BaseAlgorithm``)\n",
    "        or policy (``BasePolicy``).\n",
    "    :param env: The gym environment or ``VecEnv`` environment.\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param deterministic: Whether to use deterministic or stochastic actions\n",
    "    :param render: Whether to render the environment or not\n",
    "    :param callback: callback function to do additional checks,\n",
    "        called after each step. Gets locals() and globals() passed as parameters.\n",
    "    :param reward_threshold: Minimum expected reward per episode,\n",
    "        this will raise an error if the performance is not met\n",
    "    :param return_episode_rewards: If True, a list of rewards and episode lengths\n",
    "        per episode will be returned instead of the mean.\n",
    "    :param warn: If True (default), warns user about lack of a Monitor wrapper in the\n",
    "        evaluation environment.\n",
    "    :return: Mean reward per episode, std of reward per episode.\n",
    "        Returns ([float], [int]) when ``return_episode_rewards`` is True, first\n",
    "        list containing per-episode rewards and second containing per-episode lengths\n",
    "        (in number of steps).\n",
    "    \"\"\"\n",
    "    is_monitor_wrapped = False\n",
    "    # Avoid circular import\n",
    "    from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "    if not isinstance(env, VecEnv):\n",
    "        env = DummyVecEnv([lambda: env])  # type: ignore[list-item, return-value]\n",
    "\n",
    "    is_monitor_wrapped = is_vecenv_wrapped(env, VecMonitor) or env.env_is_wrapped(Monitor)[0]\n",
    "\n",
    "    if not is_monitor_wrapped and warn:\n",
    "        warnings.warn(\n",
    "            \"Evaluation environment is not wrapped with a ``Monitor`` wrapper. \"\n",
    "            \"This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. \"\n",
    "            \"Consider wrapping environment first with ``Monitor`` wrapper.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "\n",
    "    n_envs = env.num_envs\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    episode_counts = np.zeros(n_envs, dtype=\"int\")\n",
    "    # Divides episodes among different sub environments in the vector as evenly as possible\n",
    "    episode_count_targets = np.array([(n_eval_episodes + i) // n_envs for i in range(n_envs)], dtype=\"int\")\n",
    "\n",
    "    current_rewards = np.zeros(n_envs)\n",
    "    current_lengths = np.zeros(n_envs, dtype=\"int\")\n",
    "    observations = env.reset()\n",
    "    states = None\n",
    "    episode_starts = np.ones((env.num_envs,), dtype=bool)\n",
    "\n",
    "    # Initialize a new wandb run\n",
    "    # if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "    #     wandb.tensorboard.unpatch()\n",
    "    #wandb.tensorboard.patch(root_logdir=\"/content/drive/MyDrive/SOMN2/runs\")\n",
    "    #wandb.tensorboard.patch(root_logdir=\"./runs\")\n",
    "\n",
    "    #run = wandb.init(project='Somn_11', group='eval_15', save_code=True, reinit=True) # by_frederic\n",
    "    # run1 = wandb.init(project='Somn_11',\n",
    "    #                       config=config_PPO,\n",
    "    #                       group=f'custom-PPO-atraso_{atraso:02}',\n",
    "    #                       name=f'custom-PPO-atraso_{atraso:02}-run_{x+1:02}',\n",
    "    #                       save_code=True,\n",
    "    #                       reinit=True\n",
    "    # )\n",
    "\n",
    "    #zerou = False\n",
    "    while (episode_counts < episode_count_targets).any():\n",
    "\n",
    "\n",
    "\n",
    "        actions, states = model.predict(\n",
    "            observations,  # type: ignore[arg-type]\n",
    "            state=states,\n",
    "            episode_start=episode_starts,\n",
    "            deterministic=deterministic,\n",
    "        )\n",
    "        new_observations, rewards, dones, infos = env.step(actions)\n",
    "        current_rewards += rewards\n",
    "        current_lengths += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(n_envs):\n",
    "            if episode_counts[i] < episode_count_targets[i]:\n",
    "                # unpack values so that the callback can access the local variables\n",
    "                reward = rewards[i]\n",
    "                done = dones[i]\n",
    "                info = infos[i]\n",
    "                episode_starts[i] = done\n",
    "\n",
    "                if callback is not None:\n",
    "                    callback(locals(), globals())\n",
    "\n",
    "                if dones[i]:\n",
    "                    if is_monitor_wrapped:\n",
    "                        # Atari wrapper can send a \"done\" signal when\n",
    "                        # the agent loses a life, but it does not correspond\n",
    "                        # to the true end of episode\n",
    "                        if \"episode\" in info.keys():\n",
    "                            # Do not trust \"done\" with episode endings.\n",
    "                            # Monitor wrapper includes \"episode\" key in info if environment\n",
    "                            # has been wrapped with it. Use those rewards instead.\n",
    "                            episode_rewards.append(info[\"episode\"][\"r\"])\n",
    "                            episode_lengths.append(info[\"episode\"][\"l\"])\n",
    "                            # Only increment at the real end of an episode\n",
    "                            episode_counts[i] += 1\n",
    "                    else:\n",
    "                        episode_rewards.append(current_rewards[i])\n",
    "                        episode_lengths.append(current_lengths[i])\n",
    "                        episode_counts[i] += 1\n",
    "                    #zerou = True\n",
    "                    #wandb.finish()\n",
    "                    current_rewards[i] = 0\n",
    "                    current_lengths[i] = 0\n",
    "\n",
    "        #------------ wandb log -------------#\n",
    "        #if not zerou:\n",
    "        wandb.log({'actions_predict': actions,\n",
    "                    'reward_actions_predict': np.mean(current_rewards)\n",
    "                }\n",
    "        )\n",
    "\n",
    "        wandb.log({'reward_actions_predict': np.mean(current_rewards),\n",
    "                'actions_predict': actions\n",
    "                }\n",
    "        )\n",
    "        #------------ wandb log -------------#\n",
    "\n",
    "        observations = new_observations\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    # #------------ wandb log -------------#\n",
    "    # wandb.log({'mean_reward': np.mean(episode_rewards),\n",
    "    #            #'std_reward': np.std(episode_rewards)\n",
    "    #           }\n",
    "    # )\n",
    "    # #------------ wandb log -------------#\n",
    "    #wandb.finish()\n",
    "    if reward_threshold is not None:\n",
    "        assert mean_reward > reward_threshold, \"Mean reward below threshold: \" f\"{mean_reward:.2f} < {reward_threshold:.2f}\"\n",
    "    if return_episode_rewards:\n",
    "        return episode_rewards, episode_lengths\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7baf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import sys\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.buffers import DictRolloutBuffer, RolloutBuffer\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import obs_as_tensor, safe_mean\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "\n",
    "SelfOnPolicyAlgorithm = TypeVar(\"SelfOnPolicyAlgorithm\", bound=\"OnPolicyAlgorithm\")\n",
    "\n",
    "\n",
    "class OnPolicyAlgorithm(BaseAlgorithm):\n",
    "    \"\"\"\n",
    "    The base for On-Policy algorithms (ex: A2C/PPO).\n",
    "\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator.\n",
    "        Equivalent to classic advantage when set to 1.\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
    "        instead of action noise exploration (default: False)\n",
    "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
    "        Default: -1 (only sample at the beginning of the rollout)\n",
    "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
    "        the reported success rate, mean episode length, and mean reward over\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param monitor_wrapper: When creating an environment, whether to wrap it\n",
    "        or not in a Monitor wrapper.\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
    "        debug messages\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    :param supported_action_spaces: The action spaces supported by the algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    rollout_buffer: RolloutBuffer\n",
    "    policy: ActorCriticPolicy\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, Type[ActorCriticPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule],\n",
    "        n_steps: int,\n",
    "        gamma: float,\n",
    "        gae_lambda: float,\n",
    "        ent_coef: float,\n",
    "        vf_coef: float,\n",
    "        max_grad_norm: float,\n",
    "        use_sde: bool,\n",
    "        sde_sample_freq: int,\n",
    "        stats_window_size: int = 100,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        monitor_wrapper: bool = True,\n",
    "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "        supported_action_spaces: Optional[Tuple[Type[spaces.Space], ...]] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy=policy,\n",
    "            env=env,\n",
    "            learning_rate=learning_rate,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            support_multi_env=True,\n",
    "            seed=seed,\n",
    "            stats_window_size=stats_window_size,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            supported_action_spaces=supported_action_spaces,\n",
    "        )\n",
    "        #self.acoes\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        self._setup_lr_schedule()\n",
    "        self.set_random_seed(self.seed)\n",
    "\n",
    "        buffer_cls = DictRolloutBuffer if isinstance(self.observation_space, spaces.Dict) else RolloutBuffer\n",
    "\n",
    "        self.rollout_buffer = buffer_cls(\n",
    "            self.n_steps,\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            device=self.device,\n",
    "            gamma=self.gamma,\n",
    "            gae_lambda=self.gae_lambda,\n",
    "            n_envs=self.n_envs,\n",
    "        )\n",
    "        # pytype:disable=not-instantiable\n",
    "        self.policy = self.policy_class(  # type: ignore[assignment]\n",
    "            self.observation_space, self.action_space, self.lr_schedule, use_sde=self.use_sde, **self.policy_kwargs\n",
    "        )\n",
    "        # pytype:enable=not-instantiable\n",
    "        self.policy = self.policy.to(self.device)\n",
    "\n",
    "    def collect_rollouts(\n",
    "        self,\n",
    "        env: VecEnv,\n",
    "        callback: BaseCallback,\n",
    "        rollout_buffer: RolloutBuffer,\n",
    "        n_rollout_steps: int,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
    "        The term rollout here refers to the model-free notion and should not\n",
    "        be used with the concept of rollout used in model-based RL or planning.\n",
    "\n",
    "        :param env: The training environment\n",
    "        :param callback: Callback that will be called at each step\n",
    "            (and at the beginning and end of the rollout)\n",
    "        :param rollout_buffer: Buffer to fill with rollouts\n",
    "        :param n_rollout_steps: Number of experiences to collect per environment\n",
    "        :return: True if function returned with at least `n_rollout_steps`\n",
    "            collected, False if callback terminated rollout prematurely.\n",
    "        \"\"\"\n",
    "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
    "        # Switch to eval mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(False)\n",
    "\n",
    "        n_steps = 0\n",
    "        rollout_buffer.reset()\n",
    "        # Sample new weights for the state dependent exploration\n",
    "        if self.use_sde:\n",
    "            self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "        callback.on_rollout_start()\n",
    "\n",
    "        while n_steps < n_rollout_steps:\n",
    "            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
    "                # Sample a new noise matrix\n",
    "                self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "            with th.no_grad():\n",
    "                # Convert to pytorch tensor or to TensorDict\n",
    "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
    "                actions, values, log_probs = self.policy(obs_tensor)\n",
    "            actions = actions.cpu().numpy()\n",
    "\n",
    "            # Rescale and perform action\n",
    "            clipped_actions = actions\n",
    "            # Clip the actions to avoid out of bound error\n",
    "            if isinstance(self.action_space, spaces.Box):\n",
    "                clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "\n",
    "            new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
    "\n",
    "            self.num_timesteps += env.num_envs\n",
    "\n",
    "            # Give access to local variables\n",
    "            callback.update_locals(locals())\n",
    "            if callback.on_step() is False:\n",
    "                return False\n",
    "\n",
    "            self._update_info_buffer(infos)\n",
    "            n_steps += 1\n",
    "\n",
    "            if isinstance(self.action_space, spaces.Discrete):\n",
    "                # Reshape in case of discrete action\n",
    "                actions = actions.reshape(-1, 1)\n",
    "\n",
    "            # Handle timeout by bootstraping with value function\n",
    "            # see GitHub issue #633\n",
    "            for idx, done in enumerate(dones):\n",
    "                if (\n",
    "                    done\n",
    "                    and infos[idx].get(\"terminal_observation\") is not None\n",
    "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
    "                ):\n",
    "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
    "                    with th.no_grad():\n",
    "                        terminal_value = self.policy.predict_values(terminal_obs)[0]  # type: ignore[arg-type]\n",
    "                    rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "            rollout_buffer.add(\n",
    "                self._last_obs,  # type: ignore[arg-type]\n",
    "                actions,\n",
    "                rewards,\n",
    "                self._last_episode_starts,  # type: ignore[arg-type]\n",
    "                values,\n",
    "                log_probs,\n",
    "            )\n",
    "            self._last_obs = new_obs  # type: ignore[assignment]\n",
    "            self._last_episode_starts = dones\n",
    "\n",
    "        with th.no_grad():\n",
    "            # Compute value for the last timestep\n",
    "            values = self.policy.predict_values(obs_as_tensor(new_obs, self.device))  # type: ignore[arg-type]\n",
    "\n",
    "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "        callback.on_rollout_end()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Consume current rollout data and update policy parameters.\n",
    "        Implemented by individual algorithms.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def learn(\n",
    "        self: SelfOnPolicyAlgorithm,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 1,\n",
    "        tb_log_name: str = \"OnPolicyAlgorithm\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfOnPolicyAlgorithm:\n",
    "        iteration = 0\n",
    "\n",
    "        total_timesteps, callback = self._setup_learn(\n",
    "            total_timesteps,\n",
    "            callback,\n",
    "            reset_num_timesteps,\n",
    "            tb_log_name,\n",
    "            progress_bar,\n",
    "        )\n",
    "\n",
    "        callback.on_training_start(locals(), globals())\n",
    "\n",
    "        assert self.env is not None\n",
    "\n",
    "        #todas_acoes = []    # (by_frederic)\n",
    "\n",
    "        while self.num_timesteps < total_timesteps:\n",
    "            continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
    "\n",
    "            if continue_training is False:\n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n",
    "\n",
    "            # Display training infos\n",
    "            if log_interval is not None and iteration % log_interval == 0:\n",
    "                assert self.ep_info_buffer is not None\n",
    "                time_elapsed = max((time.time_ns() - self.start_time) / 1e9, sys.float_info.epsilon)\n",
    "                fps = int((self.num_timesteps - self._num_timesteps_at_start) / time_elapsed)\n",
    "                self.logger.record(\"time/iterations\", iteration, exclude=\"tensorboard\")\n",
    "                if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
    "                    self.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]))\n",
    "                    self.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]))\n",
    "                    # --------- WandB Log ----------- #\n",
    "                    wandb.log({\"mean_reward_test\": safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),'timesteps': self.num_timesteps})\n",
    "                    wandb.log({\"ep_len_mean\": safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]),'timesteps': self.num_timesteps})\n",
    "                self.logger.record(\"time/fps\", fps)\n",
    "                self.logger.record(\"time/time_elapsed\", int(time_elapsed), exclude=\"tensorboard\")\n",
    "                self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
    "                self.logger.dump(step=self.num_timesteps)\n",
    "                # --------- WandB Log ----------- #\n",
    "                #wandb.log({\"total_timesteps\": self.num_timesteps})\n",
    "\n",
    "                # --------- WandB Log ----------- #\n",
    "                #wandb.log({'reward': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]), 'timesteps': self.num_timesteps})\n",
    "\n",
    "                # Set up data to log in custom charts\n",
    "                #global acoes\n",
    "                #todas_acoes.append([iteration, acoes])\n",
    "\n",
    "\n",
    "\n",
    "            #del globals()['acoes']\n",
    "            self.train()\n",
    "\n",
    "\n",
    "        # Create a table with the columns to plot\n",
    "        #table = wandb.Table(data=todas_acoes, columns=[\"step\", \"acao\"])\n",
    "\n",
    "        # Use the table to populate various custom charts\n",
    "        #line_plot = wandb.plot.line(table, x='step', y='height', title='Line Plot')\n",
    "        #histogram = wandb.plot.histogram(table, value='height', title='Histogram')\n",
    "        #scatter = wandb.plot.scatter(table, x='step', y='acao', title='AcÃµes escolhidas')\n",
    "\n",
    "        # Log custom tables, which will show up in customizable charts in the UI\n",
    "        #wandb.log({#'line_1': line_plot,\n",
    "                    #'histogram_1': histogram,\n",
    "                    #'scatter_1': scatter})\n",
    "\n",
    "        callback.on_training_end()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
    "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
    "\n",
    "        return state_dicts, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f6bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance, get_schedule_fn\n",
    "\n",
    "SelfPPO = TypeVar(\"SelfPPO\", bound=\"PPO\")\n",
    "\n",
    "\n",
    "class PPO(OnPolicyAlgorithm):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
    "\n",
    "    Paper: https://arxiv.org/abs/1707.06347\n",
    "    Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
    "    https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
    "    Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
    "\n",
    "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
    "        NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
    "        See https://github.com/pytorch/pytorch/issues/29372\n",
    "    :param batch_size: Minibatch size\n",
    "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
    "        remaining (from 1 to 0).\n",
    "    :param clip_range_vf: Clipping parameter for the value function,\n",
    "        it can be a function of the current progress remaining (from 1 to 0).\n",
    "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
    "        no clipping will be done on the value function.\n",
    "        IMPORTANT: this clipping depends on the reward scaling.\n",
    "    :param normalize_advantage: Whether to normalize or not the advantage\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
    "        instead of action noise exploration (default: False)\n",
    "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
    "        Default: -1 (only sample at the beginning of the rollout)\n",
    "    :param target_kl: Limit the KL divergence between updates,\n",
    "        because the clipping is not enough to prevent large update\n",
    "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
    "        By default, there is no limit on the kl div.\n",
    "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
    "        the reported success rate, mean episode length, and mean reward over\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
    "        debug messages\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    \"\"\"\n",
    "\n",
    "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
    "        \"MlpPolicy\": ActorCriticPolicy,\n",
    "        \"CnnPolicy\": ActorCriticCnnPolicy,\n",
    "        \"MultiInputPolicy\": MultiInputActorCriticPolicy,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, Type[ActorCriticPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule] = 3e-4,\n",
    "        n_steps: int = 2048,\n",
    "        batch_size: int = 64,\n",
    "        n_epochs: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range: Union[float, Schedule] = 0.2,\n",
    "        clip_range_vf: Union[None, float, Schedule] = None,\n",
    "        normalize_advantage: bool = True,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        use_sde: bool = False,\n",
    "        sde_sample_freq: int = -1,\n",
    "        target_kl: Optional[float] = None,\n",
    "        stats_window_size: int = 100,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            stats_window_size=stats_window_size,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Sanity check, otherwise it will lead to noisy gradient and NaN\n",
    "        # because of the advantage normalization\n",
    "        if normalize_advantage:\n",
    "            assert (\n",
    "                batch_size > 1\n",
    "            ), \"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\n",
    "\n",
    "        if self.env is not None:\n",
    "            # Check that `n_steps * n_envs > 1` to avoid NaN\n",
    "            # when doing advantage normalization\n",
    "            buffer_size = self.env.num_envs * self.n_steps\n",
    "            assert buffer_size > 1 or (\n",
    "                not normalize_advantage\n",
    "            ), f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"\n",
    "            # Check that the rollout buffer size is a multiple of the mini-batch size\n",
    "            untruncated_batches = buffer_size // batch_size\n",
    "            if buffer_size % batch_size > 0:\n",
    "                warnings.warn(\n",
    "                    f\"You have specified a mini-batch size of {batch_size},\"\n",
    "                    f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"\n",
    "                    f\" after every {untruncated_batches} untruncated mini-batches,\"\n",
    "                    f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"\n",
    "                    f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"\n",
    "                    f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"\n",
    "                )\n",
    "        #self.acoes = [] # (by_frederic)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.normalize_advantage = normalize_advantage\n",
    "        self.target_kl = target_kl\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        super()._setup_model()\n",
    "\n",
    "        # Initialize schedules for policy/value clipping\n",
    "        self.clip_range = get_schedule_fn(self.clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            if isinstance(self.clip_range_vf, (float, int)):\n",
    "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, \" \"pass `None` to deactivate vf clipping\"\n",
    "\n",
    "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Update policy using the currently gathered rollout buffer.\n",
    "        \"\"\"\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update optimizer learning rate\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "        # Compute current clip range\n",
    "        clip_range = self.clip_range(self._current_progress_remaining)  # type: ignore[operator]\n",
    "        # Optional: clip range for the value function\n",
    "        if self.clip_range_vf is not None:\n",
    "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)  # type: ignore[operator]\n",
    "\n",
    "        # acoes para avaliacao (by_frederic)\n",
    "        #global acoes\n",
    "\n",
    "        entropy_losses = []\n",
    "        pg_losses, value_losses = [], []\n",
    "        clip_fractions = []\n",
    "\n",
    "        continue_training = True\n",
    "        # train for n_epochs epochs\n",
    "        for epoch in range(self.n_epochs):\n",
    "            approx_kl_divs = []\n",
    "            # Do a complete pass on the rollout buffer\n",
    "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
    "                actions = rollout_data.actions\n",
    "\n",
    "\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = rollout_data.actions.long().flatten()\n",
    "\n",
    "                # (by_frederic)\n",
    "                #self.acoes.append(actions.item())\n",
    "\n",
    "                # Re-sample the noise matrix because the log_std has changed\n",
    "                if self.use_sde:\n",
    "                    self.policy.reset_noise(self.batch_size)\n",
    "\n",
    "                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "                values = values.flatten()\n",
    "                # Normalize advantage\n",
    "                advantages = rollout_data.advantages\n",
    "                # Normalization does not make sense if mini batchsize == 1, see GH issue #325\n",
    "                if self.normalize_advantage and len(advantages) > 1:\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # ratio between old and new policy, should be one at the first iteration\n",
    "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
    "\n",
    "                # clipped surrogate loss\n",
    "                policy_loss_1 = advantages * ratio\n",
    "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "                # Logging\n",
    "                pg_losses.append(policy_loss.item())\n",
    "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n",
    "                clip_fractions.append(clip_fraction)\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    # No clipping\n",
    "                    values_pred = values\n",
    "                else:\n",
    "                    # Clip the difference between old and new value\n",
    "                    # NOTE: this depends on the reward scaling\n",
    "                    values_pred = rollout_data.old_values + th.clamp(\n",
    "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
    "                    )\n",
    "                # Value loss using the TD(gae_lambda) target\n",
    "                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
    "                value_losses.append(value_loss.item())\n",
    "\n",
    "                # acoes para avaliacao (by_frederic)\n",
    "                #acoes.append(actions.item())\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    # Approximate entropy when no analytical form\n",
    "                    entropy_loss = -th.mean(-log_prob)\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy)\n",
    "\n",
    "                entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
    "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
    "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
    "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
    "                with th.no_grad():\n",
    "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
    "                    approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
    "                    approx_kl_divs.append(approx_kl_div)\n",
    "\n",
    "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
    "                    continue_training = False\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
    "                    break\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "\n",
    "            self._n_updates += 1\n",
    "            if not continue_training:\n",
    "                break\n",
    "\n",
    "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
    "\n",
    "        # Logs\n",
    "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
    "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
    "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
    "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
    "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
    "        self.logger.record(\"train/loss\", loss.item())\n",
    "        self.logger.record(\"train/explained_variance\", explained_var)\n",
    "        if hasattr(self.policy, \"log_std\"):\n",
    "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/clip_range\", clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
    "\n",
    "        # --------- WandB Log ----------- #\n",
    "        wandb.log({'entropy_loss': np.mean(entropy_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'policy_gradient_loss': np.mean(pg_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'value_loss': np.mean(value_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'approx_kl': np.mean(approx_kl_divs), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'clip_fraction': np.mean(clip_fractions), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'loss': loss.item(), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'explained_variance': explained_var, 'timesteps': self.num_timesteps})\n",
    "\n",
    "        # Customizar um grafico para as acoes no tempo\n",
    "        acoes = actions.tolist()\n",
    "        is_None = lambda lista: lista if any(lista) else 0\n",
    "\n",
    "        wandb.log({'Actions':  np.mean(acoes),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Yard': (Yard.cont/Yard.Y)*100,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "\n",
    "        wandb.log({'Load': Demand.load,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject': Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Lead Time': np.mean(self.env.env_method('get_Demands_Attr','LT')),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'Real Lead Time': np.mean(self.env.env_method('get_Demands_Attr','real_LT')),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'Lead Time Variation': np.mean(np.array(self.env.env_method('get_Demands_Attr','real_LT')) - np.array(self.env.env_method('get_Demands_Attr','LT'))),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        atraso_log = self.env.env_method('get_atraso') if self.env.env_method('get_atraso') is not None else 0\n",
    "        wandb.log({'atraso': np.mean(atraso_log),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'atraso_demanda': np.mean(is_None(self.env.env_method('get_Demands_Attr','atraso'))),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   }\n",
    "        )\n",
    "        atraso_Demanda_class = Demand.atraso if Demand.atraso is not None else 0\n",
    "        wandb.log({'atraso_Demanda_class': atraso_Demanda_class,\n",
    "                   'timesteps': self.num_timesteps\n",
    "                   }\n",
    "        )\n",
    "\n",
    "\n",
    "        # --------- WandB Log ----------- #\n",
    "\n",
    "    def learn(\n",
    "        self: SelfPPO,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 1,\n",
    "        tb_log_name: str = \"PPO\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfPPO:\n",
    "        return super().learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=reset_num_timesteps,\n",
    "            progress_bar=progress_bar,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717d3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance, get_schedule_fn\n",
    "\n",
    "SelfPPO = TypeVar(\"SelfPPO\", bound=\"PPO\")\n",
    "\n",
    "\n",
    "class PPO(OnPolicyAlgorithm):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
    "\n",
    "    Paper: https://arxiv.org/abs/1707.06347\n",
    "    Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
    "    https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
    "    Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
    "\n",
    "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
    "        NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
    "        See https://github.com/pytorch/pytorch/issues/29372\n",
    "    :param batch_size: Minibatch size\n",
    "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
    "        remaining (from 1 to 0).\n",
    "    :param clip_range_vf: Clipping parameter for the value function,\n",
    "        it can be a function of the current progress remaining (from 1 to 0).\n",
    "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
    "        no clipping will be done on the value function.\n",
    "        IMPORTANT: this clipping depends on the reward scaling.\n",
    "    :param normalize_advantage: Whether to normalize or not the advantage\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
    "        instead of action noise exploration (default: False)\n",
    "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
    "        Default: -1 (only sample at the beginning of the rollout)\n",
    "    :param target_kl: Limit the KL divergence between updates,\n",
    "        because the clipping is not enough to prevent large update\n",
    "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
    "        By default, there is no limit on the kl div.\n",
    "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
    "        the reported success rate, mean episode length, and mean reward over\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
    "        debug messages\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    \"\"\"\n",
    "\n",
    "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
    "        \"MlpPolicy\": ActorCriticPolicy,\n",
    "        \"CnnPolicy\": ActorCriticCnnPolicy,\n",
    "        \"MultiInputPolicy\": MultiInputActorCriticPolicy,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, Type[ActorCriticPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule] = 3e-4,\n",
    "        n_steps: int = 2048,\n",
    "        batch_size: int = 64,\n",
    "        n_epochs: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range: Union[float, Schedule] = 0.2,\n",
    "        clip_range_vf: Union[None, float, Schedule] = None,\n",
    "        normalize_advantage: bool = True,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        use_sde: bool = False,\n",
    "        sde_sample_freq: int = -1,\n",
    "        target_kl: Optional[float] = None,\n",
    "        stats_window_size: int = 100,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            stats_window_size=stats_window_size,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Sanity check, otherwise it will lead to noisy gradient and NaN\n",
    "        # because of the advantage normalization\n",
    "        if normalize_advantage:\n",
    "            assert (\n",
    "                batch_size > 1\n",
    "            ), \"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\n",
    "\n",
    "        if self.env is not None:\n",
    "            # Check that `n_steps * n_envs > 1` to avoid NaN\n",
    "            # when doing advantage normalization\n",
    "            buffer_size = self.env.num_envs * self.n_steps\n",
    "            assert buffer_size > 1 or (\n",
    "                not normalize_advantage\n",
    "            ), f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"\n",
    "            # Check that the rollout buffer size is a multiple of the mini-batch size\n",
    "            untruncated_batches = buffer_size // batch_size\n",
    "            if buffer_size % batch_size > 0:\n",
    "                warnings.warn(\n",
    "                    f\"You have specified a mini-batch size of {batch_size},\"\n",
    "                    f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"\n",
    "                    f\" after every {untruncated_batches} untruncated mini-batches,\"\n",
    "                    f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"\n",
    "                    f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"\n",
    "                    f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"\n",
    "                )\n",
    "        #self.acoes = [] # (by_frederic)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.normalize_advantage = normalize_advantage\n",
    "        self.target_kl = target_kl\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        super()._setup_model()\n",
    "\n",
    "        # Initialize schedules for policy/value clipping\n",
    "        self.clip_range = get_schedule_fn(self.clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            if isinstance(self.clip_range_vf, (float, int)):\n",
    "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, \" \"pass `None` to deactivate vf clipping\"\n",
    "\n",
    "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Update policy using the currently gathered rollout buffer.\n",
    "        \"\"\"\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update optimizer learning rate\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "        # Compute current clip range\n",
    "        clip_range = self.clip_range(self._current_progress_remaining)  # type: ignore[operator]\n",
    "        # Optional: clip range for the value function\n",
    "        if self.clip_range_vf is not None:\n",
    "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)  # type: ignore[operator]\n",
    "\n",
    "        # acoes para avaliacao (by_frederic)\n",
    "        #global acoes\n",
    "\n",
    "        entropy_losses = []\n",
    "        pg_losses, value_losses = [], []\n",
    "        clip_fractions = []\n",
    "\n",
    "        continue_training = True\n",
    "        # train for n_epochs epochs\n",
    "        for epoch in range(self.n_epochs):\n",
    "            approx_kl_divs = []\n",
    "            # Do a complete pass on the rollout buffer\n",
    "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
    "                actions = rollout_data.actions\n",
    "\n",
    "\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = rollout_data.actions.long().flatten()\n",
    "\n",
    "                # (by_frederic)\n",
    "                #self.acoes.append(actions.item())\n",
    "\n",
    "                # Re-sample the noise matrix because the log_std has changed\n",
    "                if self.use_sde:\n",
    "                    self.policy.reset_noise(self.batch_size)\n",
    "\n",
    "                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "                values = values.flatten()\n",
    "                # Normalize advantage\n",
    "                advantages = rollout_data.advantages\n",
    "                # Normalization does not make sense if mini batchsize == 1, see GH issue #325\n",
    "                if self.normalize_advantage and len(advantages) > 1:\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # ratio between old and new policy, should be one at the first iteration\n",
    "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
    "\n",
    "                # clipped surrogate loss\n",
    "                policy_loss_1 = advantages * ratio\n",
    "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "                # Logging\n",
    "                pg_losses.append(policy_loss.item())\n",
    "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n",
    "                clip_fractions.append(clip_fraction)\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    # No clipping\n",
    "                    values_pred = values\n",
    "                else:\n",
    "                    # Clip the difference between old and new value\n",
    "                    # NOTE: this depends on the reward scaling\n",
    "                    values_pred = rollout_data.old_values + th.clamp(\n",
    "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
    "                    )\n",
    "                # Value loss using the TD(gae_lambda) target\n",
    "                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
    "                value_losses.append(value_loss.item())\n",
    "\n",
    "                # acoes para avaliacao (by_frederic)\n",
    "                #acoes.append(actions.item())\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    # Approximate entropy when no analytical form\n",
    "                    entropy_loss = -th.mean(-log_prob)\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy)\n",
    "\n",
    "                entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
    "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
    "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
    "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
    "                with th.no_grad():\n",
    "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
    "                    approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
    "                    approx_kl_divs.append(approx_kl_div)\n",
    "\n",
    "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
    "                    continue_training = False\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
    "                    break\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "\n",
    "            self._n_updates += 1\n",
    "            if not continue_training:\n",
    "                break\n",
    "\n",
    "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
    "\n",
    "        # Logs\n",
    "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
    "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
    "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
    "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
    "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
    "        self.logger.record(\"train/loss\", loss.item())\n",
    "        self.logger.record(\"train/explained_variance\", explained_var)\n",
    "        if hasattr(self.policy, \"log_std\"):\n",
    "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/clip_range\", clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
    "\n",
    "        # --------- WandB Log ----------- #\n",
    "        wandb.log({'entropy_loss': np.mean(entropy_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'policy_gradient_loss': np.mean(pg_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'value_loss': np.mean(value_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'approx_kl': np.mean(approx_kl_divs), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'clip_fraction': np.mean(clip_fractions), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'loss': loss.item(), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'explained_variance': explained_var, 'timesteps': self.num_timesteps})\n",
    "\n",
    "        # Customizar um grafico para as acoes no tempo\n",
    "        acoes = actions.tolist()\n",
    "        is_None = lambda lista: lista if any(lista) else 0\n",
    "\n",
    "        wandb.log({'Actions':  np.mean(acoes),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Yard': (Yard.cont/Yard.Y)*100,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "\n",
    "        wandb.log({'Load': Demand.load,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject': Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Lead Time': np.mean(self.env.env_method('get_Demands_Attr','LT')),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'Real Lead Time': np.mean(self.env.env_method('get_Demands_Attr','real_LT')),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'Lead Time Variation': np.mean(np.array(self.env.env_method('get_Demands_Attr','real_LT')) - np.array(self.env.env_method('get_Demands_Attr','LT'))),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        atraso_log = self.env.env_method('get_atraso') if self.env.env_method('get_atraso') is not None else 0\n",
    "        wandb.log({'atraso': np.mean(atraso_log),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'atraso_demanda': np.mean(is_None(self.env.env_method('get_Demands_Attr','atraso'))),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   }\n",
    "        )\n",
    "        atraso_Demanda_class = Demand.atraso if Demand.atraso is not None else 0\n",
    "        wandb.log({'atraso_Demanda_class': atraso_Demanda_class,\n",
    "                   'timesteps': self.num_timesteps\n",
    "                   }\n",
    "        )\n",
    "\n",
    "\n",
    "        # --------- WandB Log ----------- #\n",
    "\n",
    "    def learn(\n",
    "        self: SelfPPO,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 1,\n",
    "        tb_log_name: str = \"PPO\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfPPO:\n",
    "        return super().learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=reset_num_timesteps,\n",
    "            progress_bar=progress_bar,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf32098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance, get_schedule_fn\n",
    "\n",
    "SelfPPO = TypeVar(\"SelfPPO\", bound=\"PPO\")\n",
    "\n",
    "\n",
    "class PPO(OnPolicyAlgorithm):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
    "\n",
    "    Paper: https://arxiv.org/abs/1707.06347\n",
    "    Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
    "    https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
    "    Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
    "\n",
    "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
    "        NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
    "        See https://github.com/pytorch/pytorch/issues/29372\n",
    "    :param batch_size: Minibatch size\n",
    "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
    "        remaining (from 1 to 0).\n",
    "    :param clip_range_vf: Clipping parameter for the value function,\n",
    "        it can be a function of the current progress remaining (from 1 to 0).\n",
    "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
    "        no clipping will be done on the value function.\n",
    "        IMPORTANT: this clipping depends on the reward scaling.\n",
    "    :param normalize_advantage: Whether to normalize or not the advantage\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
    "        instead of action noise exploration (default: False)\n",
    "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
    "        Default: -1 (only sample at the beginning of the rollout)\n",
    "    :param target_kl: Limit the KL divergence between updates,\n",
    "        because the clipping is not enough to prevent large update\n",
    "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
    "        By default, there is no limit on the kl div.\n",
    "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
    "        the reported success rate, mean episode length, and mean reward over\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
    "        debug messages\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    \"\"\"\n",
    "\n",
    "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
    "        \"MlpPolicy\": ActorCriticPolicy,\n",
    "        \"CnnPolicy\": ActorCriticCnnPolicy,\n",
    "        \"MultiInputPolicy\": MultiInputActorCriticPolicy,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, Type[ActorCriticPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule] = 3e-4,\n",
    "        n_steps: int = 2048,\n",
    "        batch_size: int = 64,\n",
    "        n_epochs: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range: Union[float, Schedule] = 0.2,\n",
    "        clip_range_vf: Union[None, float, Schedule] = None,\n",
    "        normalize_advantage: bool = True,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        use_sde: bool = False,\n",
    "        sde_sample_freq: int = -1,\n",
    "        target_kl: Optional[float] = None,\n",
    "        stats_window_size: int = 100,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            stats_window_size=stats_window_size,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Sanity check, otherwise it will lead to noisy gradient and NaN\n",
    "        # because of the advantage normalization\n",
    "        if normalize_advantage:\n",
    "            assert (\n",
    "                batch_size > 1\n",
    "            ), \"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\n",
    "\n",
    "        if self.env is not None:\n",
    "            # Check that `n_steps * n_envs > 1` to avoid NaN\n",
    "            # when doing advantage normalization\n",
    "            buffer_size = self.env.num_envs * self.n_steps\n",
    "            assert buffer_size > 1 or (\n",
    "                not normalize_advantage\n",
    "            ), f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"\n",
    "            # Check that the rollout buffer size is a multiple of the mini-batch size\n",
    "            untruncated_batches = buffer_size // batch_size\n",
    "            if buffer_size % batch_size > 0:\n",
    "                warnings.warn(\n",
    "                    f\"You have specified a mini-batch size of {batch_size},\"\n",
    "                    f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"\n",
    "                    f\" after every {untruncated_batches} untruncated mini-batches,\"\n",
    "                    f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"\n",
    "                    f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"\n",
    "                    f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"\n",
    "                )\n",
    "        #self.acoes = [] # (by_frederic)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.normalize_advantage = normalize_advantage\n",
    "        self.target_kl = target_kl\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        super()._setup_model()\n",
    "\n",
    "        # Initialize schedules for policy/value clipping\n",
    "        self.clip_range = get_schedule_fn(self.clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            if isinstance(self.clip_range_vf, (float, int)):\n",
    "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, \" \"pass `None` to deactivate vf clipping\"\n",
    "\n",
    "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Update policy using the currently gathered rollout buffer.\n",
    "        \"\"\"\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update optimizer learning rate\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "        # Compute current clip range\n",
    "        clip_range = self.clip_range(self._current_progress_remaining)  # type: ignore[operator]\n",
    "        # Optional: clip range for the value function\n",
    "        if self.clip_range_vf is not None:\n",
    "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)  # type: ignore[operator]\n",
    "\n",
    "        # acoes para avaliacao (by_frederic)\n",
    "        #global acoes\n",
    "\n",
    "        entropy_losses = []\n",
    "        pg_losses, value_losses = [], []\n",
    "        clip_fractions = []\n",
    "\n",
    "        continue_training = True\n",
    "        # train for n_epochs epochs\n",
    "        for epoch in range(self.n_epochs):\n",
    "            approx_kl_divs = []\n",
    "            # Do a complete pass on the rollout buffer\n",
    "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
    "                actions = rollout_data.actions\n",
    "\n",
    "\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = rollout_data.actions.long().flatten()\n",
    "\n",
    "                # (by_frederic)\n",
    "                #self.acoes.append(actions.item())\n",
    "\n",
    "                # Re-sample the noise matrix because the log_std has changed\n",
    "                if self.use_sde:\n",
    "                    self.policy.reset_noise(self.batch_size)\n",
    "\n",
    "                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "                values = values.flatten()\n",
    "                # Normalize advantage\n",
    "                advantages = rollout_data.advantages\n",
    "                # Normalization does not make sense if mini batchsize == 1, see GH issue #325\n",
    "                if self.normalize_advantage and len(advantages) > 1:\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # ratio between old and new policy, should be one at the first iteration\n",
    "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
    "\n",
    "                # clipped surrogate loss\n",
    "                policy_loss_1 = advantages * ratio\n",
    "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "                # Logging\n",
    "                pg_losses.append(policy_loss.item())\n",
    "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n",
    "                clip_fractions.append(clip_fraction)\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    # No clipping\n",
    "                    values_pred = values\n",
    "                else:\n",
    "                    # Clip the difference between old and new value\n",
    "                    # NOTE: this depends on the reward scaling\n",
    "                    values_pred = rollout_data.old_values + th.clamp(\n",
    "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
    "                    )\n",
    "                # Value loss using the TD(gae_lambda) target\n",
    "                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
    "                value_losses.append(value_loss.item())\n",
    "\n",
    "                # acoes para avaliacao (by_frederic)\n",
    "                #acoes.append(actions.item())\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    # Approximate entropy when no analytical form\n",
    "                    entropy_loss = -th.mean(-log_prob)\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy)\n",
    "\n",
    "                entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
    "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
    "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
    "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
    "                with th.no_grad():\n",
    "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
    "                    approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
    "                    approx_kl_divs.append(approx_kl_div)\n",
    "\n",
    "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
    "                    continue_training = False\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
    "                    break\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "\n",
    "            self._n_updates += 1\n",
    "            if not continue_training:\n",
    "                break\n",
    "\n",
    "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
    "\n",
    "        # Logs\n",
    "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
    "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
    "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
    "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
    "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
    "        self.logger.record(\"train/loss\", loss.item())\n",
    "        self.logger.record(\"train/explained_variance\", explained_var)\n",
    "        if hasattr(self.policy, \"log_std\"):\n",
    "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/clip_range\", clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
    "\n",
    "        # --------- WandB Log ----------- #\n",
    "        wandb.log({'entropy_loss': np.mean(entropy_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'policy_gradient_loss': np.mean(pg_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'value_loss': np.mean(value_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'approx_kl': np.mean(approx_kl_divs), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'clip_fraction': np.mean(clip_fractions), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'loss': loss.item(), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'explained_variance': explained_var, 'timesteps': self.num_timesteps})\n",
    "\n",
    "        # Customizar um grafico para as acoes no tempo\n",
    "        acoes = actions.tolist()\n",
    "        is_None = lambda lista: lista if any(lista) else 0\n",
    "\n",
    "        wandb.log({'Actions':  np.mean(acoes),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Yard': (Yard.cont/Yard.Y)*100,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "\n",
    "        wandb.log({'Load': Demand.load,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject': Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Lead Time': np.mean(self.env.env_method('get_Demands_Attr','LT')),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'Real Lead Time': np.mean(self.env.env_method('get_Demands_Attr','real_LT')),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'Lead Time Variation': np.mean(np.array(self.env.env_method('get_Demands_Attr','real_LT')) - np.array(self.env.env_method('get_Demands_Attr','LT'))),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        atraso_log = self.env.env_method('get_atraso') if self.env.env_method('get_atraso') is not None else 0\n",
    "        wandb.log({'atraso': np.mean(atraso_log),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'atraso_demanda': np.mean(is_None(self.env.env_method('get_Demands_Attr','atraso'))),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   }\n",
    "        )\n",
    "        atraso_Demanda_class = Demand.atraso if Demand.atraso is not None else 0\n",
    "        wandb.log({'atraso_Demanda_class': atraso_Demanda_class,\n",
    "                   'timesteps': self.num_timesteps\n",
    "                   }\n",
    "        )\n",
    "\n",
    "\n",
    "        # --------- WandB Log ----------- #\n",
    "\n",
    "    def learn(\n",
    "        self: SelfPPO,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 1,\n",
    "        tb_log_name: str = \"PPO\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfPPO:\n",
    "        return super().learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=reset_num_timesteps,\n",
    "            progress_bar=progress_bar,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a646c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, Optional, Type, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.buffers import RolloutBuffer\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "#from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance, get_schedule_fn, obs_as_tensor, safe_mean\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "\n",
    "from sb3_contrib.common.recurrent.buffers import RecurrentDictRolloutBuffer, RecurrentRolloutBuffer\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "from sb3_contrib.common.recurrent.type_aliases import RNNStates\n",
    "from sb3_contrib.ppo_recurrent.policies import CnnLstmPolicy, MlpLstmPolicy, MultiInputLstmPolicy\n",
    "\n",
    "SelfRecurrentPPO = TypeVar(\"SelfRecurrentPPO\", bound=\"RecurrentPPO\")\n",
    "\n",
    "\n",
    "class RecurrentPPO(OnPolicyAlgorithm):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
    "    with support for recurrent policies (LSTM).\n",
    "\n",
    "    Based on the original Stable Baselines 3 implementation.\n",
    "\n",
    "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param batch_size: Minibatch size\n",
    "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
    "        remaining (from 1 to 0).\n",
    "    :param clip_range_vf: Clipping parameter for the value function,\n",
    "        it can be a function of the current progress remaining (from 1 to 0).\n",
    "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
    "        no clipping will be done on the value function.\n",
    "        IMPORTANT: this clipping depends on the reward scaling.\n",
    "    :param normalize_advantage: Whether to normalize or not the advantage\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param target_kl: Limit the KL divergence between updates,\n",
    "        because the clipping is not enough to prevent large update\n",
    "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
    "        By default, there is no limit on the kl div.\n",
    "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
    "        the reported success rate, mean episode length, and mean reward over\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: the verbosity level: 0 no output, 1 info, 2 debug\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    \"\"\"\n",
    "\n",
    "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
    "        \"MlpLstmPolicy\": MlpLstmPolicy,\n",
    "        \"CnnLstmPolicy\": CnnLstmPolicy,\n",
    "        \"MultiInputLstmPolicy\": MultiInputLstmPolicy,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, Type[RecurrentActorCriticPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule] = 3e-4,\n",
    "        n_steps: int = 128,\n",
    "        batch_size: Optional[int] = 128,\n",
    "        n_epochs: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range: Union[float, Schedule] = 0.2,\n",
    "        clip_range_vf: Union[None, float, Schedule] = None,\n",
    "        normalize_advantage: bool = True,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        use_sde: bool = False,\n",
    "        sde_sample_freq: int = -1,\n",
    "        target_kl: Optional[float] = None,\n",
    "        stats_window_size: int = 100,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            stats_window_size=stats_window_size,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            seed=seed,\n",
    "            device=device,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.normalize_advantage = normalize_advantage\n",
    "        self.target_kl = target_kl\n",
    "        self._last_lstm_states = None\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        self._setup_lr_schedule()\n",
    "        self.set_random_seed(self.seed)\n",
    "\n",
    "        buffer_cls = RecurrentDictRolloutBuffer if isinstance(self.observation_space, spaces.Dict) else RecurrentRolloutBuffer\n",
    "\n",
    "        self.policy = self.policy_class(\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            self.lr_schedule,\n",
    "            use_sde=self.use_sde,\n",
    "            **self.policy_kwargs,  # pytype:disable=not-instantiable\n",
    "        )\n",
    "        self.policy = self.policy.to(self.device)\n",
    "\n",
    "        # We assume that LSTM for the actor and the critic\n",
    "        # have the same architecture\n",
    "        lstm = self.policy.lstm_actor\n",
    "\n",
    "        if not isinstance(self.policy, RecurrentActorCriticPolicy):\n",
    "            raise ValueError(\"Policy must subclass RecurrentActorCriticPolicy\")\n",
    "\n",
    "        single_hidden_state_shape = (lstm.num_layers, self.n_envs, lstm.hidden_size)\n",
    "        # hidden and cell states for actor and critic\n",
    "        self._last_lstm_states = RNNStates(\n",
    "            (\n",
    "                th.zeros(single_hidden_state_shape, device=self.device),\n",
    "                th.zeros(single_hidden_state_shape, device=self.device),\n",
    "            ),\n",
    "            (\n",
    "                th.zeros(single_hidden_state_shape, device=self.device),\n",
    "                th.zeros(single_hidden_state_shape, device=self.device),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        hidden_state_buffer_shape = (self.n_steps, lstm.num_layers, self.n_envs, lstm.hidden_size)\n",
    "\n",
    "        self.rollout_buffer = buffer_cls(\n",
    "            self.n_steps,\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            hidden_state_buffer_shape,\n",
    "            self.device,\n",
    "            gamma=self.gamma,\n",
    "            gae_lambda=self.gae_lambda,\n",
    "            n_envs=self.n_envs,\n",
    "        )\n",
    "\n",
    "        # Initialize schedules for policy/value clipping\n",
    "        self.clip_range = get_schedule_fn(self.clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            if isinstance(self.clip_range_vf, (float, int)):\n",
    "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, pass `None` to deactivate vf clipping\"\n",
    "\n",
    "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n",
    "\n",
    "    def collect_rollouts(\n",
    "        self,\n",
    "        env: VecEnv,\n",
    "        callback: BaseCallback,\n",
    "        rollout_buffer: RolloutBuffer,\n",
    "        n_rollout_steps: int,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
    "        The term rollout here refers to the model-free notion and should not\n",
    "        be used with the concept of rollout used in model-based RL or planning.\n",
    "\n",
    "        :param env: The training environment\n",
    "        :param callback: Callback that will be called at each step\n",
    "            (and at the beginning and end of the rollout)\n",
    "        :param rollout_buffer: Buffer to fill with rollouts\n",
    "        :param n_steps: Number of experiences to collect per environment\n",
    "        :return: True if function returned with at least `n_rollout_steps`\n",
    "            collected, False if callback terminated rollout prematurely.\n",
    "        \"\"\"\n",
    "        assert isinstance(\n",
    "            rollout_buffer, (RecurrentRolloutBuffer, RecurrentDictRolloutBuffer)\n",
    "        ), f\"{rollout_buffer} doesn't support recurrent policy\"\n",
    "\n",
    "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
    "        # Switch to eval mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(False)\n",
    "\n",
    "        n_steps = 0\n",
    "        rollout_buffer.reset()\n",
    "        # Sample new weights for the state dependent exploration\n",
    "        if self.use_sde:\n",
    "            self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "        callback.on_rollout_start()\n",
    "\n",
    "        lstm_states = deepcopy(self._last_lstm_states)\n",
    "\n",
    "        while n_steps < n_rollout_steps:\n",
    "            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
    "                # Sample a new noise matrix\n",
    "                self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "            with th.no_grad():\n",
    "                # Convert to pytorch tensor or to TensorDict\n",
    "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
    "                episode_starts = th.tensor(self._last_episode_starts, dtype=th.float32, device=self.device)\n",
    "                actions, values, log_probs, lstm_states = self.policy.forward(obs_tensor, lstm_states, episode_starts)\n",
    "\n",
    "            actions = actions.cpu().numpy()\n",
    "\n",
    "            # Rescale and perform action\n",
    "            clipped_actions = actions\n",
    "            # Clip the actions to avoid out of bound error\n",
    "            if isinstance(self.action_space, spaces.Box):\n",
    "                clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "\n",
    "            new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
    "\n",
    "            self.num_timesteps += env.num_envs\n",
    "\n",
    "            # Give access to local variables\n",
    "            callback.update_locals(locals())\n",
    "            if callback.on_step() is False:\n",
    "                return False\n",
    "\n",
    "            self._update_info_buffer(infos)\n",
    "            n_steps += 1\n",
    "\n",
    "            if isinstance(self.action_space, spaces.Discrete):\n",
    "                # Reshape in case of discrete action\n",
    "                actions = actions.reshape(-1, 1)\n",
    "\n",
    "            # Handle timeout by bootstraping with value function\n",
    "            # see GitHub issue #633\n",
    "            for idx, done_ in enumerate(dones):\n",
    "                if (\n",
    "                    done_\n",
    "                    and infos[idx].get(\"terminal_observation\") is not None\n",
    "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
    "                ):\n",
    "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
    "                    with th.no_grad():\n",
    "                        terminal_lstm_state = (\n",
    "                            lstm_states.vf[0][:, idx : idx + 1, :].contiguous(),\n",
    "                            lstm_states.vf[1][:, idx : idx + 1, :].contiguous(),\n",
    "                        )\n",
    "                        # terminal_lstm_state = None\n",
    "                        episode_starts = th.tensor([False], dtype=th.float32, device=self.device)\n",
    "                        terminal_value = self.policy.predict_values(terminal_obs, terminal_lstm_state, episode_starts)[0]\n",
    "                    rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "            rollout_buffer.add(\n",
    "                self._last_obs,\n",
    "                actions,\n",
    "                rewards,\n",
    "                self._last_episode_starts,\n",
    "                values,\n",
    "                log_probs,\n",
    "                lstm_states=self._last_lstm_states,\n",
    "            )\n",
    "\n",
    "            self._last_obs = new_obs\n",
    "            self._last_episode_starts = dones\n",
    "            self._last_lstm_states = lstm_states\n",
    "\n",
    "        with th.no_grad():\n",
    "            # Compute value for the last timestep\n",
    "            episode_starts = th.tensor(dones, dtype=th.float32, device=self.device)\n",
    "            values = self.policy.predict_values(obs_as_tensor(new_obs, self.device), lstm_states.vf, episode_starts)\n",
    "\n",
    "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "        callback.on_rollout_end()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Update policy using the currently gathered rollout buffer.\n",
    "        \"\"\"\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update optimizer learning rate\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "        # Compute current clip range\n",
    "        clip_range = self.clip_range(self._current_progress_remaining)\n",
    "        # Optional: clip range for the value function\n",
    "        if self.clip_range_vf is not None:\n",
    "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)\n",
    "\n",
    "        entropy_losses = []\n",
    "        pg_losses, value_losses = [], []\n",
    "        clip_fractions = []\n",
    "\n",
    "        continue_training = True\n",
    "\n",
    "        # train for n_epochs epochs\n",
    "        for epoch in range(self.n_epochs):\n",
    "            approx_kl_divs = []\n",
    "            # Do a complete pass on the rollout buffer\n",
    "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
    "                actions = rollout_data.actions\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = rollout_data.actions.long().flatten()\n",
    "\n",
    "                # Convert mask from float to bool\n",
    "                mask = rollout_data.mask > 1e-8\n",
    "\n",
    "                # Re-sample the noise matrix because the log_std has changed\n",
    "                if self.use_sde:\n",
    "                    self.policy.reset_noise(self.batch_size)\n",
    "\n",
    "                values, log_prob, entropy = self.policy.evaluate_actions(\n",
    "                    rollout_data.observations,\n",
    "                    actions,\n",
    "                    rollout_data.lstm_states,\n",
    "                    rollout_data.episode_starts,\n",
    "                )\n",
    "\n",
    "                values = values.flatten()\n",
    "                # Normalize advantage\n",
    "                advantages = rollout_data.advantages\n",
    "                if self.normalize_advantage:\n",
    "                    advantages = (advantages - advantages[mask].mean()) / (advantages[mask].std() + 1e-8)\n",
    "\n",
    "                # ratio between old and new policy, should be one at the first iteration\n",
    "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
    "\n",
    "                # clipped surrogate loss\n",
    "                policy_loss_1 = advantages * ratio\n",
    "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                policy_loss = -th.mean(th.min(policy_loss_1, policy_loss_2)[mask])\n",
    "\n",
    "                # Logging\n",
    "                pg_losses.append(policy_loss.item())\n",
    "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()[mask]).item()\n",
    "                clip_fractions.append(clip_fraction)\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    # No clipping\n",
    "                    values_pred = values\n",
    "                else:\n",
    "                    # Clip the different between old and new value\n",
    "                    # NOTE: this depends on the reward scaling\n",
    "                    values_pred = rollout_data.old_values + th.clamp(\n",
    "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
    "                    )\n",
    "                # Value loss using the TD(gae_lambda) target\n",
    "                # Mask padded sequences\n",
    "                value_loss = th.mean(((rollout_data.returns - values_pred) ** 2)[mask])\n",
    "\n",
    "                value_losses.append(value_loss.item())\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    # Approximate entropy when no analytical form\n",
    "                    entropy_loss = -th.mean(-log_prob[mask])\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy[mask])\n",
    "\n",
    "                entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
    "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
    "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
    "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
    "                with th.no_grad():\n",
    "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
    "                    approx_kl_div = th.mean(((th.exp(log_ratio) - 1) - log_ratio)[mask]).cpu().numpy()\n",
    "                    approx_kl_divs.append(approx_kl_div)\n",
    "\n",
    "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
    "                    continue_training = False\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
    "                    break\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "\n",
    "            if not continue_training:\n",
    "                break\n",
    "\n",
    "        self._n_updates += self.n_epochs\n",
    "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
    "\n",
    "        # Logs\n",
    "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
    "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
    "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
    "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
    "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
    "        self.logger.record(\"train/loss\", loss.item())\n",
    "        self.logger.record(\"train/explained_variance\", explained_var)\n",
    "        if hasattr(self.policy, \"log_std\"):\n",
    "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/clip_range\", clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
    "\n",
    "        # --------- WandB Log ----------- #\n",
    "        wandb.log({'entropy_loss': np.mean(entropy_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'policy_gradient_loss': np.mean(pg_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'value_loss': np.mean(value_losses), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'approx_kl': np.mean(approx_kl_divs), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'clip_fraction': np.mean(clip_fractions), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'loss': loss.item(), 'timesteps': self.num_timesteps})\n",
    "        wandb.log({'explained_variance': explained_var, 'timesteps': self.num_timesteps})\n",
    "\n",
    "        # Customizar um grafico para as acoes no tempo\n",
    "        acoes = actions.tolist()\n",
    "        is_None = lambda lista: lista if any(lista) else 0\n",
    "\n",
    "        wandb.log({'Actions':  np.mean(acoes),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Yard': (Yard.cont/Yard.Y)*100,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Load': Demand.load,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject': Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Reject Total': Demand.reject_w_waste + Demand.reject,\n",
    "                   'Reject with waste': Demand.reject_w_waste,\n",
    "                   'Reject': Demand.reject,\n",
    "                   'Load': Demand.load,\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "\n",
    "        wandb.log({'Lead Time': np.mean(self.env.env_method('get_Demands_Attr','LT')),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'Real Lead Time': np.mean(self.env.env_method('get_Demands_Attr','real_LT')),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'Lead Time Variation': np.mean(np.array(self.env.env_method('get_Demands_Attr','real_LT')) - np.array(self.env.env_method('get_Demands_Attr','LT'))),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
    "                   'value_loss': np.mean(value_losses),\n",
    "                   'loss': loss.item()\n",
    "                   }\n",
    "        )\n",
    "        atraso_log = self.env.env_method('get_atraso') if self.env.env_method('get_atraso') is not None else 0\n",
    "        wandb.log({'atraso': np.mean(atraso_log),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   }\n",
    "        )\n",
    "        wandb.log({'atraso_demanda': np.mean(is_None(self.env.env_method('get_Demands_Attr','atraso'))),\n",
    "                   'timesteps': self.num_timesteps,\n",
    "                   }\n",
    "        )\n",
    "        atraso_Demanda_class = Demand.atraso if Demand.atraso is not None else 0\n",
    "        wandb.log({'atraso_Demanda_class': atraso_Demanda_class,\n",
    "                   'timesteps': self.num_timesteps\n",
    "                   }\n",
    "        )\n",
    "        # --------- WandB Log ----------- #\n",
    "\n",
    "\n",
    "    def learn(\n",
    "        self: SelfRecurrentPPO,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 1,\n",
    "        tb_log_name: str = \"RecurrentPPO\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfRecurrentPPO:\n",
    "        iteration = 0\n",
    "\n",
    "        total_timesteps, callback = self._setup_learn(\n",
    "            total_timesteps,\n",
    "            callback,\n",
    "            reset_num_timesteps,\n",
    "            tb_log_name,\n",
    "            progress_bar,\n",
    "        )\n",
    "\n",
    "        callback.on_training_start(locals(), globals())\n",
    "\n",
    "        while self.num_timesteps < total_timesteps:\n",
    "            continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
    "\n",
    "            if continue_training is False:\n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n",
    "\n",
    "            # Display training infos\n",
    "            if log_interval is not None and iteration % log_interval == 0:\n",
    "                time_elapsed = max((time.time_ns() - self.start_time) / 1e9, sys.float_info.epsilon)\n",
    "                fps = int((self.num_timesteps - self._num_timesteps_at_start) / time_elapsed)\n",
    "                self.logger.record(\"time/iterations\", iteration, exclude=\"tensorboard\")\n",
    "                if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
    "                    self.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]))\n",
    "                    self.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]))\n",
    "                    # --------- WandB Log ----------- #\n",
    "                    wandb.log({\"mean_reward_test\": safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),'timesteps': self.num_timesteps})\n",
    "                    wandb.log({\"ep_len_mean\": safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]),'timesteps': self.num_timesteps})\n",
    "                self.logger.record(\"time/fps\", fps)\n",
    "                self.logger.record(\"time/time_elapsed\", int(time_elapsed), exclude=\"tensorboard\")\n",
    "                self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
    "                self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "            self.train()\n",
    "\n",
    "        callback.on_training_end()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa3e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"jobshop class.\"\"\"\n",
    "import collections\n",
    "from ortools.sat.python import cp_model\n",
    "import wandb\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "\n",
    "#class Task:\n",
    "#    def __init__(self, machine, duration):\n",
    "#        self.machine = machine\n",
    "#        self.duration = duration\n",
    "\n",
    "class JobShop:\n",
    "    def __init__(self):\n",
    "        self.jobs_data = [ [] ]\n",
    "\n",
    "    def InsertJobs(self, job, machine_id, processing_time):\n",
    "        while len(self.jobs_data) < job+1:\n",
    "            self.jobs_data.append([])\n",
    "        self.jobs_data[job].append([machine_id, processing_time])\n",
    "        #print (self.jobs_data)\n",
    "\n",
    "    def BuildModel(self):\n",
    "        self.machines_count = 1 + max(task[0] for job in self.jobs_data for task in job)\n",
    "        self.all_machines = range(self.machines_count)\n",
    "        # Computes horizon dynamically as the sum of all durations.\n",
    "        self.horizon = sum(task[1] for job in self.jobs_data for task in job)\n",
    "\n",
    "        # Create the model.\n",
    "        self.model = cp_model.CpModel()\n",
    "\n",
    "        # Named tuple to store information about created variables.\n",
    "        self.task_type = collections.namedtuple('task_type', 'start end interval')\n",
    "        # Named tuple to manipulate solution information.\n",
    "        self.assigned_task_type = collections.namedtuple('assigned_task_type', 'start job index duration')\n",
    "\n",
    "        # Creates job intervals and add to the corresponding machine lists.\n",
    "        self.all_tasks = {}\n",
    "        self.machine_to_intervals = collections.defaultdict(list)\n",
    "\n",
    "        for job_id, job in enumerate(self.jobs_data):\n",
    "            for task_id, task in enumerate(job):\n",
    "                machine = task[0]\n",
    "                duration = task[1]\n",
    "                suffix = '_%i_%i' % (job_id, task_id)\n",
    "                start_var = self.model.NewIntVar(0, self.horizon, 'start' + suffix)\n",
    "                end_var = self.model.NewIntVar(0, self.horizon, 'end' + suffix)\n",
    "                interval_var = self.model.NewIntervalVar(start_var, duration, end_var, 'interval' + suffix)\n",
    "                self.all_tasks[job_id, task_id] = self.task_type(start=start_var, end=end_var, interval=interval_var)\n",
    "                self.machine_to_intervals[machine].append(interval_var)\n",
    "\n",
    "        # Create and add disjunctive constraints.\n",
    "        for machine in self.all_machines:\n",
    "            self.model.AddNoOverlap(self.machine_to_intervals[machine])\n",
    "\n",
    "        # Precedences inside a job.\n",
    "        for job_id, job in enumerate(self.jobs_data):\n",
    "            for task_id in range(len(job) - 1):\n",
    "                self.model.Add(self.all_tasks[job_id, task_id + 1].start >= self.all_tasks[job_id, task_id].end)\n",
    "\n",
    "        # Makespan objective.\n",
    "        self.obj_var = self.model.NewIntVar(0, self.horizon, 'makespan')\n",
    "        self.endVars = []\n",
    "        for job_id, job in enumerate(self.jobs_data):\n",
    "            self.job_length = len(job)\n",
    "            if self.job_length > 0:\n",
    "                self.endVars.append(self.all_tasks[job_id,self.job_length - 1].end)\n",
    "\n",
    "        self.model.AddMaxEquality(self.obj_var, self.endVars)\n",
    "\n",
    "        self.model.Minimize(self.obj_var)\n",
    "        # print('>>>',job_id, len(job) - 1)\n",
    "        # self.model.AddMaxEquality(obj_var, [\n",
    "        #     self.all_tasks[job_id, len(job) - 1].end\n",
    "        #     for job_id, job in enumerate(self.jobs_data)\n",
    "        # ])\n",
    "        # self.model.Minimize(obj_var)\n",
    "\n",
    "    def Solve(self):\n",
    "        # Creates the solver and solve.\n",
    "        self.solver = cp_model.CpSolver()\n",
    "        self.status = self.solver.Solve(self.model)\n",
    "\n",
    "    def Output(self):\n",
    "        if self.status == cp_model.OPTIMAL or self.status == cp_model.FEASIBLE:\n",
    "            print('Solution:')\n",
    "            # Create one list of assigned tasks per machine.\n",
    "            self.assigned_jobs = collections.defaultdict(list)\n",
    "            for job_id, job in enumerate(self.jobs_data):\n",
    "                for task_id, task in enumerate(job):\n",
    "                    machine = task[0]\n",
    "                    self.assigned_jobs[machine].append(\n",
    "                        self.assigned_task_type(start=self.solver.Value(\n",
    "                            self.all_tasks[job_id, task_id].start),\n",
    "                                           job=job_id,\n",
    "                                           index=task_id,\n",
    "                                           duration=task[1]))\n",
    "\n",
    "            # Create per machine output lines.\n",
    "            self.output = ''\n",
    "            i = 0\n",
    "            for machine in self.all_machines:\n",
    "                # Sort by starting time.\n",
    "                self.assigned_jobs[machine].sort()\n",
    "                self.sol_line_tasks = 'Machine ' + str(machine) + ': '\n",
    "                self.sol_line = '           '\n",
    "\n",
    "                for assigned_task in self.assigned_jobs[machine]:\n",
    "                    name = 'job_%i_task_%i' % (assigned_task.job,\n",
    "                                               assigned_task.index)\n",
    "                    # Add spaces to output to align columns.\n",
    "                    self.sol_line_tasks += '%-15s' % name\n",
    "\n",
    "                    start = assigned_task.start\n",
    "                    duration = assigned_task.duration\n",
    "                    sol_tmp = '[%i,%i]' % (start, start + duration)\n",
    "                    # Add spaces to output to align columns.\n",
    "                    self.sol_line += '%-15s' % sol_tmp\n",
    "\n",
    "                self.sol_line += '\\n'\n",
    "                self.sol_line_tasks += '\\n'\n",
    "                self.output += self.sol_line_tasks\n",
    "                self.output += self.sol_line\n",
    "\n",
    "            # Finally print the solution found.\n",
    "            print(f'Optimal Schedule Length: {self.solver.ObjectiveValue()}')\n",
    "            print(self.output)\n",
    "\n",
    "            wandb.log({'Objective Value': self.solver.ObjectiveValue(), 'steps': BaseAlgorithm.getNumTimeStep})\n",
    "\n",
    "        else:\n",
    "            print('No solution found.')\n",
    "\n",
    "        i += 1\n",
    "        # Statistics.\n",
    "        print('\\nStatistics')\n",
    "        print('  - conflicts: %i' % self.solver.NumConflicts())\n",
    "        print('  - branches : %i' % self.solver.NumBranches())\n",
    "        print('  - wall time: %f s' % self.solver.WallTime())\n",
    "\n",
    "# def main():\n",
    "#     instance1 = JobShop()\n",
    "#     instance1.InsertJobs(0, 0, 3)\n",
    "#     instance1.InsertJobs(0, 1, 2)\n",
    "#     instance1.InsertJobs(0, 2, 2)\n",
    "#     instance1.InsertJobs(1, 0, 2)\n",
    "#     instance1.InsertJobs(1, 2, 1)\n",
    "#     instance1.InsertJobs(1, 1, 4)\n",
    "#     instance1.InsertJobs(2, 1, 4)\n",
    "#     instance1.InsertJobs(2, 2, 3)\n",
    "#     instance1.BuildModel()\n",
    "#     instance1.Solve()\n",
    "#     instance1.Output()\n",
    "\n",
    "#     instance2 = JobShop()\n",
    "#     instance2.InsertJobs(0, 0, 5)\n",
    "#     instance2.InsertJobs(0, 1, 1)\n",
    "#     instance2.InsertJobs(0, 2, 7)\n",
    "#     instance2.InsertJobs(1, 0, 4)\n",
    "#     instance2.InsertJobs(1, 2, 2)\n",
    "#     instance2.InsertJobs(1, 1, 8)\n",
    "#     instance2.InsertJobs(2, 1, 3)\n",
    "#     instance2.InsertJobs(2, 2, 3)\n",
    "#     instance2.BuildModel()\n",
    "#     instance2.Solve()\n",
    "#     instance2.Output()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed88cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"jobshop class.\"\"\"\n",
    "import collections\n",
    "from ortools.sat.python import cp_model\n",
    "import wandb\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "\n",
    "#class Task:\n",
    "#    def __init__(self, machine, duration):\n",
    "#        self.machine = machine\n",
    "#        self.duration = duration\n",
    "\n",
    "class JobShop:\n",
    "    def __init__(self):\n",
    "        self.jobs_data = [ [] ]\n",
    "\n",
    "    def InsertJobs(self, job, machine_id, processing_time):\n",
    "        while len(self.jobs_data) < job+1:\n",
    "            self.jobs_data.append([])\n",
    "        self.jobs_data[job].append([machine_id, processing_time])\n",
    "        #print (self.jobs_data)\n",
    "\n",
    "    def BuildModel(self):\n",
    "        self.machines_count = 1 + max(task[0] for job in self.jobs_data for task in job)\n",
    "        self.all_machines = range(self.machines_count)\n",
    "        # Computes horizon dynamically as the sum of all durations.\n",
    "        self.horizon = sum(task[1] for job in self.jobs_data for task in job)\n",
    "\n",
    "        # Create the model.\n",
    "        self.model = cp_model.CpModel()\n",
    "\n",
    "        # Named tuple to store information about created variables.\n",
    "        self.task_type = collections.namedtuple('task_type', 'start end interval')\n",
    "        # Named tuple to manipulate solution information.\n",
    "        self.assigned_task_type = collections.namedtuple('assigned_task_type', 'start job index duration')\n",
    "\n",
    "        # Creates job intervals and add to the corresponding machine lists.\n",
    "        self.all_tasks = {}\n",
    "        self.machine_to_intervals = collections.defaultdict(list)\n",
    "\n",
    "        for job_id, job in enumerate(self.jobs_data):\n",
    "            for task_id, task in enumerate(job):\n",
    "                machine = task[0]\n",
    "                duration = task[1]\n",
    "                suffix = '_%i_%i' % (job_id, task_id)\n",
    "                start_var = self.model.NewIntVar(0, self.horizon, 'start' + suffix)\n",
    "                end_var = self.model.NewIntVar(0, self.horizon, 'end' + suffix)\n",
    "                interval_var = self.model.NewIntervalVar(start_var, duration, end_var, 'interval' + suffix)\n",
    "                self.all_tasks[job_id, task_id] = self.task_type(start=start_var, end=end_var, interval=interval_var)\n",
    "                self.machine_to_intervals[machine].append(interval_var)\n",
    "\n",
    "        # Create and add disjunctive constraints.\n",
    "        for machine in self.all_machines:\n",
    "            self.model.AddNoOverlap(self.machine_to_intervals[machine])\n",
    "\n",
    "        # Precedences inside a job.\n",
    "        for job_id, job in enumerate(self.jobs_data):\n",
    "            for task_id in range(len(job) - 1):\n",
    "                self.model.Add(self.all_tasks[job_id, task_id + 1].start >= self.all_tasks[job_id, task_id].end)\n",
    "\n",
    "        # Makespan objective.\n",
    "        self.obj_var = self.model.NewIntVar(0, self.horizon, 'makespan')\n",
    "        self.endVars = []\n",
    "        for job_id, job in enumerate(self.jobs_data):\n",
    "            self.job_length = len(job)\n",
    "            if self.job_length > 0:\n",
    "                self.endVars.append(self.all_tasks[job_id,self.job_length - 1].end)\n",
    "\n",
    "        self.model.AddMaxEquality(self.obj_var, self.endVars)\n",
    "\n",
    "        self.model.Minimize(self.obj_var)\n",
    "        # print('>>>',job_id, len(job) - 1)\n",
    "        # self.model.AddMaxEquality(obj_var, [\n",
    "        #     self.all_tasks[job_id, len(job) - 1].end\n",
    "        #     for job_id, job in enumerate(self.jobs_data)\n",
    "        # ])\n",
    "        # self.model.Minimize(obj_var)\n",
    "\n",
    "    def Solve(self):\n",
    "        # Creates the solver and solve.\n",
    "        self.solver = cp_model.CpSolver()\n",
    "        self.status = self.solver.Solve(self.model)\n",
    "\n",
    "    def Output(self):\n",
    "        if self.status == cp_model.OPTIMAL or self.status == cp_model.FEASIBLE:\n",
    "            print('Solution:')\n",
    "            # Create one list of assigned tasks per machine.\n",
    "            self.assigned_jobs = collections.defaultdict(list)\n",
    "            for job_id, job in enumerate(self.jobs_data):\n",
    "                for task_id, task in enumerate(job):\n",
    "                    machine = task[0]\n",
    "                    self.assigned_jobs[machine].append(\n",
    "                        self.assigned_task_type(start=self.solver.Value(\n",
    "                            self.all_tasks[job_id, task_id].start),\n",
    "                                           job=job_id,\n",
    "                                           index=task_id,\n",
    "                                           duration=task[1]))\n",
    "\n",
    "            # Create per machine output lines.\n",
    "            self.output = ''\n",
    "            i = 0\n",
    "            for machine in self.all_machines:\n",
    "                # Sort by starting time.\n",
    "                self.assigned_jobs[machine].sort()\n",
    "                self.sol_line_tasks = 'Machine ' + str(machine) + ': '\n",
    "                self.sol_line = '           '\n",
    "\n",
    "                for assigned_task in self.assigned_jobs[machine]:\n",
    "                    name = 'job_%i_task_%i' % (assigned_task.job,\n",
    "                                               assigned_task.index)\n",
    "                    # Add spaces to output to align columns.\n",
    "                    self.sol_line_tasks += '%-15s' % name\n",
    "\n",
    "                    start = assigned_task.start\n",
    "                    duration = assigned_task.duration\n",
    "                    sol_tmp = '[%i,%i]' % (start, start + duration)\n",
    "                    # Add spaces to output to align columns.\n",
    "                    self.sol_line += '%-15s' % sol_tmp\n",
    "\n",
    "                self.sol_line += '\\n'\n",
    "                self.sol_line_tasks += '\\n'\n",
    "                self.output += self.sol_line_tasks\n",
    "                self.output += self.sol_line\n",
    "\n",
    "            # Finally print the solution found.\n",
    "            print(f'Optimal Schedule Length: {self.solver.ObjectiveValue()}')\n",
    "            print(self.output)\n",
    "\n",
    "            wandb.log({'Objective Value': self.solver.ObjectiveValue(), 'steps': BaseAlgorithm.getNumTimeStep})\n",
    "\n",
    "        else:\n",
    "            print('No solution found.')\n",
    "\n",
    "        i += 1\n",
    "        # Statistics.\n",
    "        print('\\nStatistics')\n",
    "        print('  - conflicts: %i' % self.solver.NumConflicts())\n",
    "        print('  - branches : %i' % self.solver.NumBranches())\n",
    "        print('  - wall time: %f s' % self.solver.WallTime())\n",
    "\n",
    "# def main():\n",
    "#     instance1 = JobShop()\n",
    "#     instance1.InsertJobs(0, 0, 3)\n",
    "#     instance1.InsertJobs(0, 1, 2)\n",
    "#     instance1.InsertJobs(0, 2, 2)\n",
    "#     instance1.InsertJobs(1, 0, 2)\n",
    "#     instance1.InsertJobs(1, 2, 1)\n",
    "#     instance1.InsertJobs(1, 1, 4)\n",
    "#     instance1.InsertJobs(2, 1, 4)\n",
    "#     instance1.InsertJobs(2, 2, 3)\n",
    "#     instance1.BuildModel()\n",
    "#     instance1.Solve()\n",
    "#     instance1.Output()\n",
    "\n",
    "#     instance2 = JobShop()\n",
    "#     instance2.InsertJobs(0, 0, 5)\n",
    "#     instance2.InsertJobs(0, 1, 1)\n",
    "#     instance2.InsertJobs(0, 2, 7)\n",
    "#     instance2.InsertJobs(1, 0, 4)\n",
    "#     instance2.InsertJobs(1, 2, 2)\n",
    "#     instance2.InsertJobs(1, 1, 8)\n",
    "#     instance2.InsertJobs(2, 1, 3)\n",
    "#     instance2.InsertJobs(2, 2, 3)\n",
    "#     instance2.BuildModel()\n",
    "#     instance2.Solve()\n",
    "#     instance2.Output()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf4682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yard:\n",
    "    def __init__(self, Y, numFeat, typFeat):\n",
    "        Yard.Y=Y\n",
    "        Yard.yard = [0 for _ in range(numFeat)]\n",
    "        Yard.cont=0\n",
    "        Yard.space = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a181fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "class Demand:\n",
    "\n",
    "    cont=0      # by_frederic ---> mudei de 1 para 0\n",
    "    load=0\n",
    "    reject=0         # rejeitar\n",
    "    reject_w_waste=0 # rejeitar com lixo\n",
    "    #atraso=None\n",
    "\n",
    "    # Somn(Y=10,M=10,N=10,MAXDO=10,MAXAM=3,MAXPR=2,MAXPE=10,MAXFT=5,MAXMT=3,MAXTI=2,MAXEU = 5, atraso=atraso)\n",
    "    def __init__(self,\n",
    "                 M:int,\n",
    "                 N:int,\n",
    "                 MAXDO:int,\n",
    "                 MAXAM:int,\n",
    "                 MAXPR:float,\n",
    "                 MAXPE:int,\n",
    "                 MAXFT:int,\n",
    "                 MAXMT:int,\n",
    "                 MAXTI:int,\n",
    "                 MAXEU:int,\n",
    "                 t: int,\n",
    "                 atraso: int = None):\n",
    "\n",
    "        Demand.M=M\n",
    "        Demand.N=N\n",
    "        Demand.MAXDO=MAXDO\n",
    "        Demand.MAXAM=MAXAM\n",
    "        Demand.MAXPR=MAXPR\n",
    "        Demand.MAXPE=MAXPE\n",
    "        Demand.MAXFT=MAXFT\n",
    "        Demand.MAXMT=MAXMT\n",
    "        Demand.MAXTI=MAXTI\n",
    "        Demand.MAXEU=MAXEU\n",
    "        Demand.EU = np.random.random(M)*MAXEU\n",
    "        self.ST = int(-1)                  ### free(-1) received(0), ready(1), rejected(2), produced(3), stored(4) and delivered(5)\n",
    "        Demand.cont +=1\n",
    "        Demand.atraso=atraso\n",
    "\n",
    "\n",
    "    def __call__(self, t:int):\n",
    "\n",
    "        self.CU = Demand.cont\n",
    "    #   self.PR = random.randrange(3,Demand.MAXPR)  below -----------------\n",
    "        self.AM = random.randrange(1,Demand.MAXAM)\n",
    "        self.PE = random.randint(1,Demand.MAXPE)\n",
    "        self.ST = int(0)                  ###received0, ready1, rejected2, produced3, stored4 and delivered5\n",
    "        self.FT = np.random.randint(0,Demand.MAXFT,self.M)\n",
    "        if not np.any(self.FT):\n",
    "            self.FT[0] = 1      ## by_frederic\n",
    "\n",
    "        ### Tempo ###\n",
    "        self.F = 0\n",
    "        for i in range(self.M):\n",
    "            self.F += int(self.FT[i]>0)\n",
    "\n",
    "        self.LT = int(self.F/2) + 2                      ###  --- 1.0*self.fun_tau() * self.F\n",
    "        if self.atraso >=0:   ### NÃ£o Ã© mais None (-1 para habilitar poisson)\n",
    "            self.real_LT = self.LT + self.atraso\n",
    "        else:\n",
    "            self.real_LT = poisson.rvs(mu=self.LT) # by_frederic\n",
    "        self.DI = t\n",
    "        self.DO = t + self.LT + random.randint(0,Demand.MAXDO)\n",
    "\n",
    "        self.CO = 0.0\n",
    "        for j in range(Demand.M):\n",
    "            self.CO += self.FT[j] * Demand.EU[j]\n",
    "        #self.CO = self.AM * self.CO  -- custo sem o amount\n",
    "        self.PR = Demand.MAXPR*self.CO  ### LUCRO EH 2X CUSTO  self.PR = Demand.MAXPE  (by fred)\n",
    "\n",
    "        self.SP = self.fun_gamma() ####* 'cpu'.Y   #SPACE CONSUMPTION FACTOR\n",
    "        self.VA = self.fun_upsilon() ### [0low 1up]\n",
    "        self.SU = 1- self.fun_sigma() ### [0low 1up]\n",
    "        self.TP = self.DO - t\n",
    "\n",
    "\n",
    "    def fun_gamma(self) -> float:\n",
    "        x = (self.AM*self.F)/(Demand.MAXAM * self.M)\n",
    "        return x\n",
    "\n",
    "    def fun_tau(self) -> float:\n",
    "        x = (self.AM*self.F)/(Demand.MAXAM * self.M)\n",
    "        return x\n",
    "\n",
    "    def fun_upsilon(self) -> float:\n",
    "        x = self.F/self.M\n",
    "        return x\n",
    "\n",
    "    def fun_sigma(self) -> float:\n",
    "        x = self.F/self.M\n",
    "        return x\n",
    "\n",
    "#  def fun_beta(self, IN, OU) -> float:\n",
    "#    x=0\n",
    "#    for i in range(self.M):\n",
    "#      if IN[i]==OU[i]:\n",
    "#        x+=1\n",
    "#    x = x/self.M\n",
    "#    return x\n",
    "\n",
    "#  def calculate_statics(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b7ba06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text(0, 0.5, 'P(X=k)')"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import poisson\n",
    "\n",
    "#atraso = poisson.rvs(mu=2, size=100, random_state=0)\n",
    "atraso = [poisson.rvs(mu=4) for _ in range(1000)]\n",
    "sns.histplot(atraso, discrete=True)\n",
    "plt.xlim([-1,15])\n",
    "plt.xlabel('Atraso (k)')\n",
    "plt.ylabel('P(X=k)')\n",
    "#atraso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e121f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(atraso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5655ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "poisson.rvs(mu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09eaf307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random.mtrand import seed\n",
    "# a biblioteca gym mudou\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces  # Discrete, Box, Tuple,  Dict\n",
    "from gymnasium import Env\n",
    "from heapdict import heapdict\n",
    "\n",
    "# biblioteca stable_baselines3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "#from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "# from stable_baselines3 import PPO, A2C, DQN, DDPG, TD3\n",
    "\n",
    "# outras bibliotecas\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "from absl import flags\n",
    "from scipy.stats import poisson\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "class Somn(Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        M: int,\n",
    "        N: int,\n",
    "        Y: int,\n",
    "        MAXDO: int,\n",
    "        MAXAM: int,\n",
    "        MAXPR: int,\n",
    "        MAXPE: int,\n",
    "        MAXFT: int,\n",
    "        MAXMT: int,\n",
    "        MAXTI: int,\n",
    "        MAXEU: int,\n",
    "        #seed: int,\n",
    "        atraso: int = None\n",
    "    ):\n",
    "        super(Somn).__init__()\n",
    "\n",
    "        Somn.priorqpr = heapdict()\n",
    "        # Somn.instance = JobShop()\n",
    "        # Somn.priorqsu = heapdict()\n",
    "        # Somn.priorqva = heapdict()\n",
    "        Somn.time = 1\n",
    "        Somn.objetivo = 1\n",
    "\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        self.Y = Y\n",
    "        self.MAXDO = MAXDO\n",
    "        self.MAXAM = MAXAM\n",
    "        self.MAXPR = MAXPR\n",
    "        self.MAXPE = MAXPE\n",
    "        self.MAXFT = MAXFT\n",
    "        self.MAXMT = MAXMT\n",
    "        self.MAXTI = MAXTI\n",
    "        self.MAXEU = MAXEU\n",
    "        # self.MT = np.random.randint(0,MAXFT,M)\n",
    "        self.EU = np.random.random(M) * MAXEU\n",
    "        self.BA = np.random.randint(0, MAXFT, M)\n",
    "        self.IN = np.random.randint(0, MAXFT, M)\n",
    "        self.OU = np.random.randint(0, MAXFT, M)\n",
    "        #self.seed = seed\n",
    "        self.atraso = atraso # (by_frederic)\n",
    "        # self.state = np.zeros((N,5))\n",
    "\n",
    "        # print('Inicializado', M, N , Y)\n",
    "\n",
    "        self.DE = [\n",
    "            Demand(\n",
    "                M, N, MAXDO, MAXAM, MAXPR, MAXPE, MAXFT, MAXMT, MAXTI, MAXEU, Somn.time, self.atraso\n",
    "            )\n",
    "            for _ in range(N)\n",
    "        ]\n",
    "        self.YA = [Yard(Y, M, MAXFT) for _ in range(Y)]\n",
    "\n",
    "        ######################\n",
    "        #      lb e ub       #\n",
    "        ######################\n",
    "        \"\"\"\n",
    "        (lb=lowerbound ub=upperbound) para o espaco de Observacao e Acao\n",
    "        \"\"\"\n",
    "\n",
    "        # ST varia de -2 a 5\n",
    "        self.lb_ST = -2\n",
    "        self.ub_ST = 5\n",
    "        # time varia de 1 a (10*MAXDO + M)\n",
    "        self.lb_time = 1\n",
    "        self.ub_time = 10 * self.MAXDO + self.M\n",
    "        # LT varia de 2 a (M/2 + 2)\n",
    "        self.lb_LT = 2\n",
    "#        self.ub_LT = int(self.M / 2) + 2    #### ACMO LT AFETADO POR LT(M) + CARGA(N)\n",
    "        self.ub_LT = self.M + self.N\n",
    "        # DO varia de 3 a (ub_time + ub_LT + MAXDO)\n",
    "        self.lb_DO = 3\n",
    "        self.ub_DO = self.ub_time + self.ub_LT + self.MAXDO\n",
    "\n",
    "\n",
    "        # TP varia de 2 a (ub_time + ub_LT + 2) onde 2 e um ruido (troquei 2 pela distribuicao de poisson)\n",
    "        self.lb_TP = 2\n",
    "        p = [poisson.rvs(mu=self.ub_LT) for _ in range(10000)]\n",
    "        self.ub_TP = self.ub_time + self.ub_LT + max(p)\n",
    "\n",
    "\n",
    "        # MT varia de 0 a MAXFT\n",
    "        self.lb_MT = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
    "        self.ub_MT = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
    "        # EU varia de 0 a MAXEU\n",
    "        self.lb_EU = np.array([0 for _ in range(self.M)]).astype(np.float64)\n",
    "        self.ub_EU = np.array([MAXEU for _ in range(self.M)]).astype(np.float64)\n",
    "        # BA varia de 0 a MAXFT\n",
    "        self.lb_BA = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
    "        self.ub_BA = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
    "        # IN varia de 0 a MAXFT\n",
    "        self.lb_IN = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
    "        self.ub_IN = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
    "        # OU varia de 0 a MAXFT\n",
    "        self.lb_OU = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
    "        self.ub_OU = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
    "\n",
    "        # lb e ub--- segunda versao (sem a coluna com os valores de Somn.time)\n",
    "        # self.lb = np.array([[self.lb_ST, self.lb_LT, self.lb_DO, self.lb_TP] for _ in range(self.N)])\n",
    "        # self.ub = np.array([[self.ub_ST, self.ub_LT, self.ub_DO, self.ub_TP] for _ in range(self.N)])\n",
    "\n",
    "        # # lb e ub--- primeira versao (com a coluna com os valores de Somn.time)\n",
    "        # self.lb = np.array([[self.lb_ST, self.lb_time, self.lb_LT, self.lb_DO, self.lb_TP] for _ in range(self.N)])\n",
    "        # self.ub = np.array([[self.ub_ST, self.ub_time, self.ub_LT, self.ub_DO, self.ub_TP] for _ in range(self.N)])\n",
    "\n",
    "\n",
    "\n",
    "        ######################\n",
    "        #      Espacos       #\n",
    "        ######################\n",
    "        \"\"\"\n",
    "        Precisa mudar o espaco de acao\n",
    "        de acordo com o algoritmo utilizado\n",
    "        \"\"\"\n",
    "\n",
    "        # accept to produce or reject\n",
    "        # self.action_space = spaces.Box(0, 4, shape=(1,)) # usar o TD3\n",
    "        self.action_space = spaces.Discrete(self.MAXDO)  # usar com o PPO, DQN, A2C\n",
    "\n",
    "        # Espaco de observacao (como ficam as demandas depois da acao)\n",
    "        # self.observation_space = spaces.Box(self.lb, self.ub, dtype=int)\n",
    "        # self.observation_space = spaces.Dict({'tempo':spaces.Box(self.lb_time, self.ub_time, shape=(1,), dtype=int),\n",
    "        #                                 'estado': spaces.Box(self.lb, self.ub, dtype=int)})      # versao para MultiInputPolicy\n",
    "        # self.observation_space = spaces.Dict({'time':spaces.Box(self.lb_time, self.ub_time, shape=(1,), dtype=int),\n",
    "        #                                'MT': spaces.Box(self.lb_MT, self.ub_MT, dtype=int),\n",
    "        #                                'EU': spaces.Box(self.lb_EU, self.ub_EU, dtype=float),\n",
    "        #                                'BA': spaces.Box(self.lb_BA, self.ub_BA, dtype=int),\n",
    "        #                                'IN': spaces.Box(self.lb_IN, self.ub_IN, dtype=int),\n",
    "        #                                'OU': spaces.Box(self.lb_OU, self.ub_OU, dtype=int),\n",
    "        #                                'state': spaces.Box(self.lb, self.ub, dtype=int)})          # versao para MultiInputPolicy\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"time\": spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float64),\n",
    "                \"MT\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"EU\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"BA\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"IN\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"OU\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"state\": spaces.Box(\n",
    "                    low=0.0, high=1.0, shape=(self.N, 4), dtype=np.float64\n",
    "                ),\n",
    "            }\n",
    "        )  # versao para MultiInputPolicy Normalizada\n",
    "\n",
    "\n",
    "    ######################\n",
    "    #      funcoes       #\n",
    "    ######################\n",
    "\n",
    "    # recebe um atributo por exemplo 'LT' ou 'real_LT' e devolve os valores\n",
    "    # de Lead Time das Demandas ou Real Lead Time, conforme o atributo passado.\n",
    "    def get_Demands_Attr(env, atributo):\n",
    "        d = [getattr(demandas, atributo) for demandas in env.DE]\n",
    "        return d\n",
    "\n",
    "    def get_atraso(self):\n",
    "        d = self.atraso\n",
    "        return d\n",
    "\n",
    "    # Normaliza o valor dentro do range passado como parametro\n",
    "    def normaliza(self, x, min, max):\n",
    "        x_norm = np.array((x - min) / (max - min)).astype(np.float64)\n",
    "        return x_norm\n",
    "\n",
    "    def readDemand(self):\n",
    "        for i in range(Demand.N):\n",
    "            if (\n",
    "                self.DE[i].ST == -1\n",
    "            ):  # or self.DE[i].ST == 0: ZERO nÃ£o pode ser status de livre\n",
    "                self.DE[i](Somn.time)\n",
    "\n",
    "    def match_demand_with_inventory(self, limiar: float) -> bool:\n",
    "        matched = False\n",
    "        for i in range(Demand.N):\n",
    "          if self.DE[i].ST == 0: ## SÃ PODE DAR MATCH DEMANDAS CHEGADAS\n",
    "            for y in range(Yard.cont):\n",
    "                match = 0\n",
    "                # print('Y...', y, 'YA=', YA[y].yard,Yard.cont, 'l=', limiar)\n",
    "                for j in range(Demand.M):\n",
    "                    # print('Y(y,j):', y,j, 'Y x D:', self.YA[y].yard[j],self.DE[i].FT[j], 'cont:', Yard.cont, 'l x m:', limiar, match)\n",
    "                    if self.DE[i].FT[j] > 0:\n",
    "                        if self.DE[i].FT[j] <= self.YA[y].yard[j]:\n",
    "                            match = match + 1\n",
    "                    # se for ZERO entÃ£o nÃ£o pode ter a caracteristica\n",
    "                    else:\n",
    "                        if self.YA[y].yard[j] == 0:\n",
    "                            match = match + 1\n",
    "\n",
    "                if match >= limiar:\n",
    "                    # print(\"\\n Match: Casou\", Yard.cont)\n",
    "                    self.YA[y].yard = self.YA[Yard.cont - 1].yard  ## apaga o registro de match com o Ãºltimo da lista\n",
    "                    Yard.cont -= 1    ### FRED NAO DEIXAR BAIXAR DE ZERO\n",
    "                    self.DE[i].ST = 5  ## delivered p/ contar lucro\n",
    "                    matched = True\n",
    "        return matched\n",
    "\n",
    "    def product_schedulingold(self, t: int, action):\n",
    "        for i in range(self.N):\n",
    "            if self.DE[i].ST == 1:\n",
    "                if self.DE[i].DO > (t + self.DE[i].LT + action):\n",
    "                    self.DE[i].ST = 3  ## produced status --- remember to run time for each case\n",
    "                    self.OU -= self.DE[i].FT  ## CONSOME OS RECURSOS\n",
    "                    ################################################\n",
    "                    #                                              #\n",
    "                    #                  atraso                      #\n",
    "                    #                                              #\n",
    "                    ################################################\n",
    "\n",
    "                    #Somn.producing = min(self.M, Somn.producing + 1)\n",
    "                    #Somn.producing = Somn.producing + 1\n",
    "                    #Demand.load = Demand.load + 1\n",
    "                    #self.DE[i].TP = t + self.DE[i].LT + poisson.rvs(mu=2) -- LOAD NA CONTA\n",
    "                    if self.atraso >= 0:\n",
    "                        self.DE[i].real_LT = self.DE[i].LT + self.atraso\n",
    "                    else:\n",
    "                        self.DE[i].real_LT = poisson.rvs(mu=(self.DE[i].LT+Demand.load)) # by_frederic\n",
    "\n",
    "                    self.DE[i].TP = t + self.DE[i].real_LT\n",
    "\n",
    "                    # if self.atraso > 5:\n",
    "                    #     self.DE[i].TP = t + self.DE[i].LT + poisson.rvs(mu=2)\n",
    "                    # else:\n",
    "                    #     self.DE[i].TP = t + self.DE[i].LT + self.atraso  # ruido  --- trocar por distribuiÃ§Ã£o poison --- ou por algo que dependa de AM random.randint(1,Demand.MAXTI)\n",
    "                    #     # print('\\n **** PRODUCED because', self.DE[i].DO, '>', t + self.DE[i].LT + action)\n",
    "                else:\n",
    "                    #Somn.producing = max(0, Somn.producing - 1)\n",
    "                    #Somn.producing = Somn.producing - 1\n",
    "                    self.DE[i].ST = 2  ## rejected status\n",
    "                    self.OU -= self.DE[i].FT  ### libera do buffer de produÃ§Ã£o\n",
    "                    self.BA += self.DE[i].FT  ## devolve para o saldo para os prÃ³ximos\n",
    "                    # print('\\n **** REJECTED by DO', self.DE[i].DO, ' <= DI+LT+act', t , self.DE[i].LT , action)\n",
    "\n",
    "    def product_scheduling(self, t: int, action):\n",
    "        flag = 0\n",
    "        for _ in range(len(Somn.priorqpr)):  ### ACMO UTILIZAR 3 FILAS E ESCOLHER UMA DELAS AQUI\n",
    "            obj = Somn.priorqpr.popitem()\n",
    "            i = obj[0]\n",
    "            if i > 0:\n",
    "                if self.DE[i].ST == 1:  ## DE[I].ST VAI SER SEMPRE 1 PORQUE VEM DA FILAP\n",
    "### COPY JOB TO JOBSHOP SCHEDULING\n",
    "                #   for j in range(self.M):\n",
    "                #     if self.DE[i].FT[j]!= 0:\n",
    "                #     #   Somn.instance.InsertJobs(i, j, self.DE[i].FT[j])\n",
    "                #       flag = 1\n",
    "###\n",
    "                    if self.DE[i].DO > (t + self.DE[i].LT + action):\n",
    "                        self.DE[i].ST = 3  ## produced status --- remember to run time for each case\n",
    "                        self.OU -= self.DE[i].FT  ## CONSOME OS RECURSOS\n",
    "                        Demand.load = Demand.load + 1\n",
    "                        self.DE[i].real_LT = poisson.rvs(mu=(self.DE[i].LT+Demand.load)) # by_frederic\n",
    "                        self.DE[i].TP = t + self.DE[i].real_LT\n",
    "                    else:\n",
    "                        Demand.reject = Demand.reject + 1\n",
    "                        self.DE[i].ST = 2  ## rejected status\n",
    "                        self.OU -= self.DE[i].FT  ### libera do buffer de produÃ§Ã£o\n",
    "                        self.BA += self.DE[i].FT  ## devolve para o saldo para os prÃ³ximos\n",
    "## se formou buffer, resolve para comparar depois\n",
    "        # if flag ==1:\n",
    "        #   Somn.instance.BuildModel()\n",
    "        #   Somn.instance.Solve()\n",
    "        #   Somn.instance.Output()  ## precisa salvar a lista de resultados\n",
    "\n",
    "\n",
    "    def product_destination(self, t: int):\n",
    "        for i in range(Demand.N):\n",
    "            if self.DE[i].ST == 3:\n",
    "                if self.DE[i].TP < t:  ### TP eh resultado de LT(#f) + RAND\n",
    "                    #Somn.producing = Somn.producing - 1\n",
    "                    Demand.load = Demand.load - 1\n",
    "                    if t < self.DE[i].DO:\n",
    "                        self.DE[i].ST = 5  ## produced status --- remember to run time for each case\n",
    "                        # print(\"\\n Destination: Enviou\", Yard.cont)\n",
    "                    else:\n",
    "                        self.DE[i].ST = 4  ## stored status\n",
    "                        if Yard.cont < Yard.Y - 1:\n",
    "                            self.YA[Yard.cont].yard = self.DE[i].FT\n",
    "                            Yard.cont += 1\n",
    "                            # print(\"\\n Destination: Armazenou no YARD\", Yard.cont)\n",
    "                        else:\n",
    "                            self.DE[i].ST = -2  ## NAO CABE ... REJEITADO COM GERAÃÃO DE LIXO (CASO MAIS GRAVE)\n",
    "                            Demand.reject_w_waste = Demand.reject_w_waste + 1\n",
    "\n",
    "    def stock_covers_demand(self):\n",
    "        covered = True\n",
    "        for i in range(self.N):\n",
    "            if self.DE[i].ST == 0:\n",
    "                DF = self.BA - self.DE[i].FT\n",
    "                OR = np.array(\n",
    "                    [abs(i) if i < 0 else 0 for i in DF]\n",
    "                )  # O QUE PRECISA SER COMPRADO\n",
    "                # print('\\n ORDER from ', DF, ':', OR)\n",
    "                if not np.any(OR):\n",
    "                    self.DE[i].ST = 1\n",
    "\n",
    "## ACMO SETAR A PRIORIDADE\n",
    "# BY PROFIT -- escolher um ???? e comentar o outro\n",
    "#                    Somn.priorqpr[i] = int(1/(self.DE[i].AM * self.DE[i].PR))\n",
    "# BY SUSTAIN\n",
    "#                    Somn.priorqsu[i] = 1 - int(self.DE[i].SU)  ## [0low 1up]\n",
    "# BY VARIATI\n",
    "#                    Somn.priorqva[i] = 1 - int(self.DE[i].VA)  ## [0low 1up]\n",
    "                    # if self.object == 1:\n",
    "                    #   Somn.priorqpr[i] = 1/(self.DE[i].AM * self.DE[i].PR)\n",
    "                    # elif self.object == 2:\n",
    "                    #   Somn.priorqsu[i] = 1 - self.DE[i].SU  ## [0low 1up]\n",
    "                    # else:\n",
    "                    #   Somn.priorqva[i] = 1 - self.DE[i].VA  ## [0low 1up]\n",
    "# STOCK ISSUES\n",
    "\n",
    " #                   Somn.priorqpr[i] = (1 - self.DE[i].SU)\n",
    "                    Somn.priorqpr[i] = 1/(self.DE[i].AM * self.DE[i].PR)\n",
    "                    # print ((1 - self.DE[i].SU))\n",
    "                    self.BA -= np.array(DF)  ### ATUALIZA O SALDO\n",
    "                    self.OU += np.array(DF)  ### ATUALIZA A SAÃDA\n",
    "                    # print('\\n balance:', self.BA,  'because not buying',self.OU)\n",
    "                else:\n",
    "                    covered = False\n",
    "                    self.IN += np.array(OR)  ## ATUALIZA O TOTAL DE COMPRAVEIS\n",
    "                    # print('\\n balance: ', self.BA, 'because buying',OR, 'accumulating', self.IN)\n",
    "        return covered\n",
    "\n",
    "    # def order_raw_material(self, t: int):\n",
    "    # self.IN = [random.randint(0,i) if i > 0 else 0 for i in self.IN]\n",
    "    # return self.IN\n",
    "\n",
    "    def eval_final_states(self) -> float:\n",
    "        totReward = 0.0\n",
    "        totPenalty = 0.0\n",
    "        for i in range(self.N):\n",
    "            if self.DE[i].ST == 2:\n",
    "                totPenalty += 0\n",
    "                # print('REJECTED vvvvvvvvvvvvvvvvvvvvvvvvvvvv')\n",
    "            if self.DE[i].ST == -2:\n",
    "                totPenalty += self.DE[i].AM * self.DE[i].CO\n",
    "                # print('PREJUIZO $$$$$$$$$$$$$$$$$$$$$$$$$')\n",
    "            if self.DE[i].ST == 4:\n",
    "                totPenalty += totReward / (\n",
    "                    Yard.space - Yard.cont + 1\n",
    "                )  ### penalidade inversamente proporcional ao espaÃ§o remanescente\n",
    "                # print('STORED <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "            if self.DE[i].ST == 5:\n",
    "## ACMO AJUSTAR O REWARD DE ACORDO COM A PRIORIDADE\n",
    "#                totReward += self.DE[i].AM * self.DE[i].PR\n",
    "#                totReward += self.DE[i].AM * self.DE[i].SU\n",
    "#                totReward += self.DE[i].AM * self.DE[i].VA\n",
    "#                 if self.object == 1:\n",
    "#                   totReward += self.DE[i].AM * self.DE[i].PR\n",
    "#                 elif self.object == 2:\n",
    "#                   totReward += self.DE[i].AM * self.DE[i].PR\n",
    "# #                    totReward += self.DE[i].AM * self.DE[i].SU\n",
    "#                 else:\n",
    "#                   totReward += self.DE[i].AM * self.DE[i].VA\n",
    "                # print('REWARD ******************************')\n",
    "                totReward += self.DE[i].AM * self.DE[i].PR\n",
    "        self.DE[i].ST = -1  # LIBERA O ESPAÃO APÃS CONTABILIZADO\n",
    "        return totReward, totPenalty\n",
    "\n",
    "    ######################\n",
    "    #       step         #\n",
    "    ######################\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Atualiza tudo aqui e devolve o prÃ³ximo estado: n_state, reward, done, info\n",
    "\n",
    "            - n_state: prÃ³ximo estado;\n",
    "            - reward: recompensa da aÃ§Ã£o;\n",
    "            - done: flag de conclusÃ£o;\n",
    "            - info: informaÃµes extras (opcional)\n",
    "\n",
    "        Primeira versÃ£o vai fazer uma iteraÃ§Ã£o para cada episÃ³dio ...\n",
    "        O Tempo t precisa ser controlado\n",
    "        \"\"\"\n",
    "\n",
    "        # receive RAW MATERIAL AND ORDERS (DEMANDS)\n",
    "        self.MT = np.array([random.randint(0, i) if i > 0 else 0 for i in self.IN])\n",
    "        self.readDemand()\n",
    "\n",
    "        # IF PREVIOUS ORDERS INVENTORY AVAILABLE, PLEASE DISPATCH\n",
    "        self.match_demand_with_inventory(self.MAXFT)\n",
    "        #    self.product_destination(Somn.time)\n",
    "\n",
    "        # ANYWAY, UPDATE BALANCE AND INCOME RAW MATERIAL REGARDING MT RECEIVED\n",
    "        self.IN -= self.MT\n",
    "        self.BA += self.MT\n",
    "\n",
    "        # IF RAW MATERIAL INVENTORY DOES NOT COVER PLEASE REQUEST RAW MATERIAL\n",
    "        if not self.stock_covers_demand():\n",
    "            self.IN = np.array(\n",
    "                [random.randint(0, i) if i > 0 else 0 for i in self.IN]\n",
    "            ).astype(np.int64)\n",
    "\n",
    "        # ANYWAY START PRODUCING AND DISPATCHING\n",
    "        self.product_scheduling(Somn.time, action)\n",
    "        self.product_destination(Somn.time)\n",
    "        Somn.time += 1\n",
    "\n",
    "        # ORDINARY PROCEDURES IN STEP METHOD INCLUDING REWARD BY INSPECTING FINAL STATES\n",
    "        # 1 STATE\n",
    "        arrayState = []\n",
    "\n",
    "        for i in range(self.N):\n",
    "            aux_row = [\n",
    "                self.normaliza(x=self.DE[i].ST, min=self.lb_ST, max=self.ub_ST),\n",
    "                # Somn.time,\n",
    "                # self.DE[i].SP,\n",
    "                self.normaliza(self.DE[i].LT, self.lb_LT, self.ub_LT),\n",
    "                # self.DE[i].VA,\n",
    "                # self.DE[i].SU,\n",
    "                # self.DE[i].PR,\n",
    "                self.normaliza(self.DE[i].DO, self.lb_DO, self.ub_DO),\n",
    "                self.normaliza(self.DE[i].TP, self.lb_TP, self.ub_TP),\n",
    "            ]\n",
    "            arrayState.append(aux_row)\n",
    "\n",
    "        self.state = np.array(arrayState)\n",
    "\n",
    "        # 2 REWARD\n",
    "        (\n",
    "            reward,\n",
    "            penalty\n",
    "        ) = self.eval_final_states()  # aqui vai a funÃ§Ã£o que calcula a recompensa\n",
    "\n",
    "        # Gera grafico do Yard (by_frederic)\n",
    "\n",
    "\n",
    "        # 3 FINAL CONDITION\n",
    "        done = False\n",
    "        truncated = False\n",
    "        # if penalty>0:\n",
    "        # reward =0\n",
    "        # print('\\n D -- O -- N -- E --', self.state)\n",
    "        # done = True\n",
    "\n",
    "        if Somn.time >= self.ub_time:  # 10*Demand.MAXDO + Demand.M   (TEMPOMAX)\n",
    "            # print('\\n D -- O -- N -- E --', self.state)\n",
    "            done = True\n",
    "\n",
    "        # Atualiza o upper bounds\n",
    "\n",
    "        self.ub_MT = max(self.MAXFT, np.amax(self.MT))\n",
    "        self.ub_BA = max(self.MAXFT, np.amax(self.BA))\n",
    "        self.ub_IN = max(self.MAXFT, np.amax(self.IN))\n",
    "\n",
    "        info = {}  # InformaÃ§Ãµes adicionais\n",
    "        # observation = self.state  #by_frederic: retorna quando e um tipo Box\n",
    "        observation = {\n",
    "            \"time\": np.array([self.normaliza(self.time, self.lb_time, self.ub_time)]),\n",
    "            \"MT\": self.normaliza(self.MT, self.lb_MT, self.ub_MT),\n",
    "            \"EU\": self.normaliza(self.EU, self.lb_EU, self.ub_EU),\n",
    "            \"BA\": self.normaliza(self.BA, self.lb_BA, self.ub_BA),\n",
    "            \"IN\": self.normaliza(self.IN, self.lb_IN, self.ub_IN),\n",
    "            \"OU\": self.normaliza(self.OU, self.lb_OU, self.ub_OU),\n",
    "            \"state\": self.state,\n",
    "        }  # by_frederic: retorna quando e um tipo Dict\n",
    "\n",
    "        return (\n",
    "            observation,\n",
    "            reward,\n",
    "            done,\n",
    "            truncated,\n",
    "            info,\n",
    "        )  # , exprofit   # by_frederic:\n",
    "\n",
    "    ######################\n",
    "    #       reset        #\n",
    "    ######################\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        #super().reset(seed=None)\n",
    "        Somn.priorqpr = heapdict()\n",
    "        # Somn.priorqsu = heapdict()\n",
    "        # Somn.priorqva = heapdict()\n",
    "        self.MT = np.random.randint(0, self.MAXFT, self.M)\n",
    "        self.EU = np.random.random(self.M) * self.MAXEU\n",
    "        self.BA = np.random.randint(0, self.MAXFT, self.M)\n",
    "        self.IN = np.random.randint(0, self.MAXFT, self.M)\n",
    "        self.OU = np.random.randint(0, self.MAXFT, self.M)\n",
    "        Somn.time = 1\n",
    "        Demand.load = 1\n",
    "        Demand.reject = 0\n",
    "        Demand.reject_w_waste=0\n",
    "\n",
    "        self.YA = [Yard(self.Y, self.M, self.MAXFT) for _ in range(self.Y)]\n",
    "\n",
    "        arrayState = []\n",
    "        for i in range(self.N):\n",
    "            self.DE[i](Somn.time)\n",
    "            aux_row = [\n",
    "                self.normaliza(x=self.DE[i].ST, min=self.lb_ST, max=self.ub_ST),\n",
    "                # Somn.time,\n",
    "                # self.DE[i].SP,\n",
    "                self.normaliza(x=self.DE[i].LT, min=self.lb_LT, max=self.ub_LT),\n",
    "                # self.DE[i].VA,\n",
    "                # self.DE[i].SU,\n",
    "                # self.DE[i].PR,\n",
    "                self.normaliza(x=self.DE[i].DO, min=self.lb_DO, max=self.ub_DO),\n",
    "                self.normaliza(x=self.DE[i].TP, min=self.lb_TP, max=self.ub_TP),\n",
    "            ]\n",
    "            arrayState.append(aux_row)\n",
    "\n",
    "        self.state = np.array(arrayState)\n",
    "\n",
    "        info = dict()\n",
    "        # observation = (self.state, info)  # by_frederic: retorna quando o tipo Ã© Box\n",
    "        observation = {\n",
    "            \"time\": np.array([self.normaliza(self.time, self.lb_time, self.ub_time)]),\n",
    "            \"MT\": self.normaliza(self.MT, self.lb_MT, self.ub_MT),\n",
    "            \"EU\": self.normaliza(self.EU, self.lb_EU, self.ub_EU),\n",
    "            \"BA\": self.normaliza(self.BA, self.lb_BA, self.ub_BA),\n",
    "            \"IN\": self.normaliza(self.IN, self.lb_IN, self.ub_IN),\n",
    "            \"OU\": self.normaliza(self.OU, self.lb_OU, self.ub_OU),\n",
    "            \"state\": self.state,\n",
    "        }  # by_frederic: retorna quando e um tipo Dict\n",
    "\n",
    "        return (observation, info)  # by_frederic: para se adequar ao Gymnasium\n",
    "\n",
    "    ######################\n",
    "    #       render       #\n",
    "    ######################\n",
    "\n",
    "    def render(self):\n",
    "        # print(\"Current state (RENDER): \\n\", self.state)\n",
    "        pass\n",
    "\n",
    "    ######################\n",
    "    #       close        #\n",
    "    ######################\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f97a5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random.mtrand import seed\n",
    "# a biblioteca gym mudou\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces  # Discrete, Box, Tuple,  Dict\n",
    "from gymnasium import Env\n",
    "from heapdict import heapdict\n",
    "\n",
    "# biblioteca stable_baselines3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "#from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "# from stable_baselines3 import PPO, A2C, DQN, DDPG, TD3\n",
    "\n",
    "# outras bibliotecas\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "from absl import flags\n",
    "from scipy.stats import poisson\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "class Somn(Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        M: int,\n",
    "        N: int,\n",
    "        Y: int,\n",
    "        MAXDO: int,\n",
    "        MAXAM: int,\n",
    "        MAXPR: int,\n",
    "        MAXPE: int,\n",
    "        MAXFT: int,\n",
    "        MAXMT: int,\n",
    "        MAXTI: int,\n",
    "        MAXEU: int,\n",
    "        #seed: int,\n",
    "        atraso: int = None\n",
    "    ):\n",
    "        super(Somn).__init__()\n",
    "\n",
    "        Somn.priorqpr = heapdict()\n",
    "        # Somn.instance = JobShop()\n",
    "        # Somn.priorqsu = heapdict()\n",
    "        # Somn.priorqva = heapdict()\n",
    "        Somn.time = 1\n",
    "        Somn.objetivo = 1\n",
    "\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        self.Y = Y\n",
    "        self.MAXDO = MAXDO\n",
    "        self.MAXAM = MAXAM\n",
    "        self.MAXPR = MAXPR\n",
    "        self.MAXPE = MAXPE\n",
    "        self.MAXFT = MAXFT\n",
    "        self.MAXMT = MAXMT\n",
    "        self.MAXTI = MAXTI\n",
    "        self.MAXEU = MAXEU\n",
    "        # self.MT = np.random.randint(0,MAXFT,M)\n",
    "        self.EU = np.random.random(M) * MAXEU\n",
    "        self.BA = np.random.randint(0, MAXFT, M)\n",
    "        self.IN = np.random.randint(0, MAXFT, M)\n",
    "        self.OU = np.random.randint(0, MAXFT, M)\n",
    "        #self.seed = seed\n",
    "        self.atraso = atraso # (by_frederic)\n",
    "        # self.state = np.zeros((N,5))\n",
    "\n",
    "        # print('Inicializado', M, N , Y)\n",
    "\n",
    "        self.DE = [\n",
    "            Demand(\n",
    "                M, N, MAXDO, MAXAM, MAXPR, MAXPE, MAXFT, MAXMT, MAXTI, MAXEU, Somn.time, self.atraso\n",
    "            )\n",
    "            for _ in range(N)\n",
    "        ]\n",
    "        self.YA = [Yard(Y, M, MAXFT) for _ in range(Y)]\n",
    "\n",
    "        ######################\n",
    "        #      lb e ub       #\n",
    "        ######################\n",
    "        \"\"\"\n",
    "        (lb=lowerbound ub=upperbound) para o espaco de Observacao e Acao\n",
    "        \"\"\"\n",
    "\n",
    "        # ST varia de -2 a 5\n",
    "        self.lb_ST = -2\n",
    "        self.ub_ST = 5\n",
    "        # time varia de 1 a (10*MAXDO + M)\n",
    "        self.lb_time = 1\n",
    "        self.ub_time = 10 * self.MAXDO + self.M\n",
    "        # LT varia de 2 a (M/2 + 2)\n",
    "        self.lb_LT = 2\n",
    "#        self.ub_LT = int(self.M / 2) + 2    #### ACMO LT AFETADO POR LT(M) + CARGA(N)\n",
    "        self.ub_LT = self.M + self.N\n",
    "        # DO varia de 3 a (ub_time + ub_LT + MAXDO)\n",
    "        self.lb_DO = 3\n",
    "        self.ub_DO = self.ub_time + self.ub_LT + self.MAXDO\n",
    "\n",
    "\n",
    "        # TP varia de 2 a (ub_time + ub_LT + 2) onde 2 e um ruido (troquei 2 pela distribuicao de poisson)\n",
    "        self.lb_TP = 2\n",
    "        p = [poisson.rvs(mu=self.ub_LT) for _ in range(10000)]\n",
    "        self.ub_TP = self.ub_time + self.ub_LT + max(p)\n",
    "\n",
    "\n",
    "        # MT varia de 0 a MAXFT\n",
    "        self.lb_MT = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
    "        self.ub_MT = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
    "        # EU varia de 0 a MAXEU\n",
    "        self.lb_EU = np.array([0 for _ in range(self.M)]).astype(np.float64)\n",
    "        self.ub_EU = np.array([MAXEU for _ in range(self.M)]).astype(np.float64)\n",
    "        # BA varia de 0 a MAXFT\n",
    "        self.lb_BA = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
    "        self.ub_BA = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
    "        # IN varia de 0 a MAXFT\n",
    "        self.lb_IN = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
    "        self.ub_IN = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
    "        # OU varia de 0 a MAXFT\n",
    "        self.lb_OU = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
    "        self.ub_OU = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
    "\n",
    "        # lb e ub--- segunda versao (sem a coluna com os valores de Somn.time)\n",
    "        # self.lb = np.array([[self.lb_ST, self.lb_LT, self.lb_DO, self.lb_TP] for _ in range(self.N)])\n",
    "        # self.ub = np.array([[self.ub_ST, self.ub_LT, self.ub_DO, self.ub_TP] for _ in range(self.N)])\n",
    "\n",
    "        # # lb e ub--- primeira versao (com a coluna com os valores de Somn.time)\n",
    "        # self.lb = np.array([[self.lb_ST, self.lb_time, self.lb_LT, self.lb_DO, self.lb_TP] for _ in range(self.N)])\n",
    "        # self.ub = np.array([[self.ub_ST, self.ub_time, self.ub_LT, self.ub_DO, self.ub_TP] for _ in range(self.N)])\n",
    "\n",
    "\n",
    "\n",
    "        ######################\n",
    "        #      Espacos       #\n",
    "        ######################\n",
    "        \"\"\"\n",
    "        Precisa mudar o espaco de acao\n",
    "        de acordo com o algoritmo utilizado\n",
    "        \"\"\"\n",
    "\n",
    "        # accept to produce or reject\n",
    "        # self.action_space = spaces.Box(0, 4, shape=(1,)) # usar o TD3\n",
    "        self.action_space = spaces.Discrete(self.MAXDO)  # usar com o PPO, DQN, A2C\n",
    "\n",
    "        # Espaco de observacao (como ficam as demandas depois da acao)\n",
    "        # self.observation_space = spaces.Box(self.lb, self.ub, dtype=int)\n",
    "        # self.observation_space = spaces.Dict({'tempo':spaces.Box(self.lb_time, self.ub_time, shape=(1,), dtype=int),\n",
    "        #                                 'estado': spaces.Box(self.lb, self.ub, dtype=int)})      # versao para MultiInputPolicy\n",
    "        # self.observation_space = spaces.Dict({'time':spaces.Box(self.lb_time, self.ub_time, shape=(1,), dtype=int),\n",
    "        #                                'MT': spaces.Box(self.lb_MT, self.ub_MT, dtype=int),\n",
    "        #                                'EU': spaces.Box(self.lb_EU, self.ub_EU, dtype=float),\n",
    "        #                                'BA': spaces.Box(self.lb_BA, self.ub_BA, dtype=int),\n",
    "        #                                'IN': spaces.Box(self.lb_IN, self.ub_IN, dtype=int),\n",
    "        #                                'OU': spaces.Box(self.lb_OU, self.ub_OU, dtype=int),\n",
    "        #                                'state': spaces.Box(self.lb, self.ub, dtype=int)})          # versao para MultiInputPolicy\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"time\": spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float64),\n",
    "                \"MT\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"EU\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"BA\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"IN\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"OU\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
    "                \"state\": spaces.Box(\n",
    "                    low=0.0, high=1.0, shape=(self.N, 4), dtype=np.float64\n",
    "                ),\n",
    "            }\n",
    "        )  # versao para MultiInputPolicy Normalizada\n",
    "\n",
    "\n",
    "    ######################\n",
    "    #      funcoes       #\n",
    "    ######################\n",
    "\n",
    "    # recebe um atributo por exemplo 'LT' ou 'real_LT' e devolve os valores\n",
    "    # de Lead Time das Demandas ou Real Lead Time, conforme o atributo passado.\n",
    "    def get_Demands_Attr(env, atributo):\n",
    "        d = [getattr(demandas, atributo) for demandas in env.DE]\n",
    "        return d\n",
    "\n",
    "    def get_atraso(self):\n",
    "        d = self.atraso\n",
    "        return d\n",
    "\n",
    "    # Normaliza o valor dentro do range passado como parametro\n",
    "    def normaliza(self, x, min, max):\n",
    "        x_norm = np.array((x - min) / (max - min)).astype(np.float64)\n",
    "        return x_norm\n",
    "\n",
    "    def readDemand(self):\n",
    "        for i in range(Demand.N):\n",
    "            if (\n",
    "                self.DE[i].ST == -1\n",
    "            ):  # or self.DE[i].ST == 0: ZERO nÃ£o pode ser status de livre\n",
    "                self.DE[i](Somn.time)\n",
    "\n",
    "    def match_demand_with_inventory(self, limiar: float) -> bool:\n",
    "        matched = False\n",
    "        for i in range(Demand.N):\n",
    "          if self.DE[i].ST == 0: ## SÃ PODE DAR MATCH DEMANDAS CHEGADAS\n",
    "            for y in range(Yard.cont):\n",
    "                match = 0\n",
    "                # print('Y...', y, 'YA=', YA[y].yard,Yard.cont, 'l=', limiar)\n",
    "                for j in range(Demand.M):\n",
    "                    # print('Y(y,j):', y,j, 'Y x D:', self.YA[y].yard[j],self.DE[i].FT[j], 'cont:', Yard.cont, 'l x m:', limiar, match)\n",
    "                    if self.DE[i].FT[j] > 0:\n",
    "                        if self.DE[i].FT[j] <= self.YA[y].yard[j]:\n",
    "                            match = match + 1\n",
    "                    # se for ZERO entÃ£o nÃ£o pode ter a caracteristica\n",
    "                    else:\n",
    "                        if self.YA[y].yard[j] == 0:\n",
    "                            match = match + 1\n",
    "\n",
    "                if match >= limiar:\n",
    "                    # print(\"\\n Match: Casou\", Yard.cont)\n",
    "                    self.YA[y].yard = self.YA[Yard.cont - 1].yard  ## apaga o registro de match com o Ãºltimo da lista\n",
    "                    Yard.cont -= 1    ### FRED NAO DEIXAR BAIXAR DE ZERO\n",
    "                    self.DE[i].ST = 5  ## delivered p/ contar lucro\n",
    "                    matched = True\n",
    "        return matched\n",
    "\n",
    "    def product_schedulingold(self, t: int, action):\n",
    "        for i in range(self.N):\n",
    "            if self.DE[i].ST == 1:\n",
    "                if self.DE[i].DO > (t + self.DE[i].LT + action):\n",
    "                    self.DE[i].ST = 3  ## produced status --- remember to run time for each case\n",
    "                    self.OU -= self.DE[i].FT  ## CONSOME OS RECURSOS\n",
    "                    ################################################\n",
    "                    #                                              #\n",
    "                    #                  atraso                      #\n",
    "                    #                                              #\n",
    "                    ################################################\n",
    "\n",
    "                    #Somn.producing = min(self.M, Somn.producing + 1)\n",
    "                    #Somn.producing = Somn.producing + 1\n",
    "                    #Demand.load = Demand.load + 1\n",
    "                    #self.DE[i].TP = t + self.DE[i].LT + poisson.rvs(mu=2) -- LOAD NA CONTA\n",
    "                    if self.atraso >= 0:\n",
    "                        self.DE[i].real_LT = self.DE[i].LT + self.atraso\n",
    "                    else:\n",
    "                        self.DE[i].real_LT = poisson.rvs(mu=(self.DE[i].LT+Demand.load)) # by_frederic\n",
    "\n",
    "                    self.DE[i].TP = t + self.DE[i].real_LT\n",
    "\n",
    "                    # if self.atraso > 5:\n",
    "                    #     self.DE[i].TP = t + self.DE[i].LT + poisson.rvs(mu=2)\n",
    "                    # else:\n",
    "                    #     self.DE[i].TP = t + self.DE[i].LT + self.atraso  # ruido  --- trocar por distribuiÃ§Ã£o poison --- ou por algo que dependa de AM random.randint(1,Demand.MAXTI)\n",
    "                    #     # print('\\n **** PRODUCED because', self.DE[i].DO, '>', t + self.DE[i].LT + action)\n",
    "                else:\n",
    "                    #Somn.producing = max(0, Somn.producing - 1)\n",
    "                    #Somn.producing = Somn.producing - 1\n",
    "                    self.DE[i].ST = 2  ## rejected status\n",
    "                    self.OU -= self.DE[i].FT  ### libera do buffer de produÃ§Ã£o\n",
    "                    self.BA += self.DE[i].FT  ## devolve para o saldo para os prÃ³ximos\n",
    "                    # print('\\n **** REJECTED by DO', self.DE[i].DO, ' <= DI+LT+act', t , self.DE[i].LT , action)\n",
    "\n",
    "    def product_scheduling(self, t: int, action):\n",
    "        flag = 0\n",
    "        for _ in range(len(Somn.priorqpr)):  ### ACMO UTILIZAR 3 FILAS E ESCOLHER UMA DELAS AQUI\n",
    "            obj = Somn.priorqpr.popitem()\n",
    "            i = obj[0]\n",
    "            if i > 0:\n",
    "                if self.DE[i].ST == 1:  ## DE[I].ST VAI SER SEMPRE 1 PORQUE VEM DA FILAP\n",
    "### COPY JOB TO JOBSHOP SCHEDULING\n",
    "                #   for j in range(self.M):\n",
    "                #     if self.DE[i].FT[j]!= 0:\n",
    "                #     #   Somn.instance.InsertJobs(i, j, self.DE[i].FT[j])\n",
    "                #       flag = 1\n",
    "###\n",
    "                    if self.DE[i].DO > (t + self.DE[i].LT + action):\n",
    "                        self.DE[i].ST = 3  ## produced status --- remember to run time for each case\n",
    "                        self.OU -= self.DE[i].FT  ## CONSOME OS RECURSOS\n",
    "                        Demand.load = Demand.load + 1\n",
    "                        self.DE[i].real_LT = poisson.rvs(mu=(self.DE[i].LT+Demand.load)) # by_frederic\n",
    "                        self.DE[i].TP = t + self.DE[i].real_LT\n",
    "                    else:\n",
    "                        Demand.reject = Demand.reject + 1\n",
    "                        self.DE[i].ST = 2  ## rejected status\n",
    "                        self.OU -= self.DE[i].FT  ### libera do buffer de produÃ§Ã£o\n",
    "                        self.BA += self.DE[i].FT  ## devolve para o saldo para os prÃ³ximos\n",
    "## se formou buffer, resolve para comparar depois\n",
    "        # if flag ==1:\n",
    "        #   Somn.instance.BuildModel()\n",
    "        #   Somn.instance.Solve()\n",
    "        #   Somn.instance.Output()  ## precisa salvar a lista de resultados\n",
    "\n",
    "\n",
    "    def product_destination(self, t: int):\n",
    "        for i in range(Demand.N):\n",
    "            if self.DE[i].ST == 3:\n",
    "                if self.DE[i].TP < t:  ### TP eh resultado de LT(#f) + RAND\n",
    "                    #Somn.producing = Somn.producing - 1\n",
    "                    Demand.load = Demand.load - 1\n",
    "                    if t < self.DE[i].DO:\n",
    "                        self.DE[i].ST = 5  ## produced status --- remember to run time for each case\n",
    "                        # print(\"\\n Destination: Enviou\", Yard.cont)\n",
    "                    else:\n",
    "                        self.DE[i].ST = 4  ## stored status\n",
    "                        if Yard.cont < Yard.Y - 1:\n",
    "                            self.YA[Yard.cont].yard = self.DE[i].FT\n",
    "                            Yard.cont += 1\n",
    "                            # print(\"\\n Destination: Armazenou no YARD\", Yard.cont)\n",
    "                        else:\n",
    "                            self.DE[i].ST = -2  ## NAO CABE ... REJEITADO COM GERAÃÃO DE LIXO (CASO MAIS GRAVE)\n",
    "                            Demand.reject_w_waste = Demand.reject_w_waste + 1\n",
    "\n",
    "    def stock_covers_demand(self):\n",
    "        covered = True\n",
    "        for i in range(self.N):\n",
    "            if self.DE[i].ST == 0:\n",
    "                DF = self.BA - self.DE[i].FT\n",
    "                OR = np.array(\n",
    "                    [abs(i) if i < 0 else 0 for i in DF]\n",
    "                )  # O QUE PRECISA SER COMPRADO\n",
    "                # print('\\n ORDER from ', DF, ':', OR)\n",
    "                if not np.any(OR):\n",
    "                    self.DE[i].ST = 1\n",
    "\n",
    "## ACMO SETAR A PRIORIDADE\n",
    "# BY PROFIT -- escolher um ???? e comentar o outro\n",
    "#                    Somn.priorqpr[i] = int(1/(self.DE[i].AM * self.DE[i].PR))\n",
    "# BY SUSTAIN\n",
    "#                    Somn.priorqsu[i] = 1 - int(self.DE[i].SU)  ## [0low 1up]\n",
    "# BY VARIATI\n",
    "#                    Somn.priorqva[i] = 1 - int(self.DE[i].VA)  ## [0low 1up]\n",
    "                    # if self.object == 1:\n",
    "                    #   Somn.priorqpr[i] = 1/(self.DE[i].AM * self.DE[i].PR)\n",
    "                    # elif self.object == 2:\n",
    "                    #   Somn.priorqsu[i] = 1 - self.DE[i].SU  ## [0low 1up]\n",
    "                    # else:\n",
    "                    #   Somn.priorqva[i] = 1 - self.DE[i].VA  ## [0low 1up]\n",
    "# STOCK ISSUES\n",
    "\n",
    " #                   Somn.priorqpr[i] = (1 - self.DE[i].SU)\n",
    "                    Somn.priorqpr[i] = 1/(self.DE[i].AM * self.DE[i].PR)\n",
    "                    # print ((1 - self.DE[i].SU))\n",
    "                    self.BA -= np.array(DF)  ### ATUALIZA O SALDO\n",
    "                    self.OU += np.array(DF)  ### ATUALIZA A SAÃDA\n",
    "                    # print('\\n balance:', self.BA,  'because not buying',self.OU)\n",
    "                else:\n",
    "                    covered = False\n",
    "                    self.IN += np.array(OR)  ## ATUALIZA O TOTAL DE COMPRAVEIS\n",
    "                    # print('\\n balance: ', self.BA, 'because buying',OR, 'accumulating', self.IN)\n",
    "        return covered\n",
    "\n",
    "    # def order_raw_material(self, t: int):\n",
    "    # self.IN = [random.randint(0,i) if i > 0 else 0 for i in self.IN]\n",
    "    # return self.IN\n",
    "\n",
    "    def eval_final_states(self) -> float:\n",
    "        totReward = 0.0\n",
    "        totPenalty = 0.0\n",
    "        for i in range(self.N):\n",
    "            if self.DE[i].ST == 2:\n",
    "                totPenalty += 0\n",
    "                # print('REJECTED vvvvvvvvvvvvvvvvvvvvvvvvvvvv')\n",
    "            if self.DE[i].ST == -2:\n",
    "                totPenalty += self.DE[i].AM * self.DE[i].CO\n",
    "                # print('PREJUIZO $$$$$$$$$$$$$$$$$$$$$$$$$')\n",
    "            if self.DE[i].ST == 4:\n",
    "                totPenalty += totReward / (\n",
    "                    Yard.space - Yard.cont + 1\n",
    "                )  ### penalidade inversamente proporcional ao espaÃ§o remanescente\n",
    "                # print('STORED <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "            if self.DE[i].ST == 5:\n",
    "## ACMO AJUSTAR O REWARD DE ACORDO COM A PRIORIDADE\n",
    "#                totReward += self.DE[i].AM * self.DE[i].PR\n",
    "#                totReward += self.DE[i].AM * self.DE[i].SU\n",
    "#                totReward += self.DE[i].AM * self.DE[i].VA\n",
    "#                 if self.object == 1:\n",
    "#                   totReward += self.DE[i].AM * self.DE[i].PR\n",
    "#                 elif self.object == 2:\n",
    "#                   totReward += self.DE[i].AM * self.DE[i].PR\n",
    "# #                    totReward += self.DE[i].AM * self.DE[i].SU\n",
    "#                 else:\n",
    "#                   totReward += self.DE[i].AM * self.DE[i].VA\n",
    "                # print('REWARD ******************************')\n",
    "                totReward += self.DE[i].AM * self.DE[i].PR\n",
    "        self.DE[i].ST = -1  # LIBERA O ESPAÃO APÃS CONTABILIZADO\n",
    "        return totReward, totPenalty\n",
    "\n",
    "    ######################\n",
    "    #       step         #\n",
    "    ######################\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Atualiza tudo aqui e devolve o prÃ³ximo estado: n_state, reward, done, info\n",
    "\n",
    "            - n_state: prÃ³ximo estado;\n",
    "            - reward: recompensa da aÃ§Ã£o;\n",
    "            - done: flag de conclusÃ£o;\n",
    "            - info: informaÃµes extras (opcional)\n",
    "\n",
    "        Primeira versÃ£o vai fazer uma iteraÃ§Ã£o para cada episÃ³dio ...\n",
    "        O Tempo t precisa ser controlado\n",
    "        \"\"\"\n",
    "\n",
    "        # receive RAW MATERIAL AND ORDERS (DEMANDS)\n",
    "        self.MT = np.array([random.randint(0, i) if i > 0 else 0 for i in self.IN])\n",
    "        self.readDemand()\n",
    "\n",
    "        # IF PREVIOUS ORDERS INVENTORY AVAILABLE, PLEASE DISPATCH\n",
    "        self.match_demand_with_inventory(self.MAXFT)\n",
    "        #    self.product_destination(Somn.time)\n",
    "\n",
    "        # ANYWAY, UPDATE BALANCE AND INCOME RAW MATERIAL REGARDING MT RECEIVED\n",
    "        self.IN -= self.MT\n",
    "        self.BA += self.MT\n",
    "\n",
    "        # IF RAW MATERIAL INVENTORY DOES NOT COVER PLEASE REQUEST RAW MATERIAL\n",
    "        if not self.stock_covers_demand():\n",
    "            self.IN = np.array(\n",
    "                [random.randint(0, i) if i > 0 else 0 for i in self.IN]\n",
    "            ).astype(np.int64)\n",
    "\n",
    "        # ANYWAY START PRODUCING AND DISPATCHING\n",
    "        self.product_scheduling(Somn.time, action)\n",
    "        self.product_destination(Somn.time)\n",
    "        Somn.time += 1\n",
    "\n",
    "        # ORDINARY PROCEDURES IN STEP METHOD INCLUDING REWARD BY INSPECTING FINAL STATES\n",
    "        # 1 STATE\n",
    "        arrayState = []\n",
    "\n",
    "        for i in range(self.N):\n",
    "            aux_row = [\n",
    "                self.normaliza(x=self.DE[i].ST, min=self.lb_ST, max=self.ub_ST),\n",
    "                # Somn.time,\n",
    "                # self.DE[i].SP,\n",
    "                self.normaliza(self.DE[i].LT, self.lb_LT, self.ub_LT),\n",
    "                # self.DE[i].VA,\n",
    "                # self.DE[i].SU,\n",
    "                # self.DE[i].PR,\n",
    "                self.normaliza(self.DE[i].DO, self.lb_DO, self.ub_DO),\n",
    "                self.normaliza(self.DE[i].TP, self.lb_TP, self.ub_TP),\n",
    "            ]\n",
    "            arrayState.append(aux_row)\n",
    "\n",
    "        self.state = np.array(arrayState)\n",
    "\n",
    "        # 2 REWARD\n",
    "        (\n",
    "            reward,\n",
    "            penalty\n",
    "        ) = self.eval_final_states()  # aqui vai a funÃ§Ã£o que calcula a recompensa\n",
    "\n",
    "        # Gera grafico do Yard (by_frederic)\n",
    "\n",
    "\n",
    "        # 3 FINAL CONDITION\n",
    "        done = False\n",
    "        truncated = False\n",
    "        # if penalty>0:\n",
    "        # reward =0\n",
    "        # print('\\n D -- O -- N -- E --', self.state)\n",
    "        # done = True\n",
    "\n",
    "        if Somn.time >= self.ub_time:  # 10*Demand.MAXDO + Demand.M   (TEMPOMAX)\n",
    "            # print('\\n D -- O -- N -- E --', self.state)\n",
    "            done = True\n",
    "\n",
    "        # Atualiza o upper bounds\n",
    "\n",
    "        self.ub_MT = max(self.MAXFT, np.amax(self.MT))\n",
    "        self.ub_BA = max(self.MAXFT, np.amax(self.BA))\n",
    "        self.ub_IN = max(self.MAXFT, np.amax(self.IN))\n",
    "\n",
    "        info = {}  # InformaÃ§Ãµes adicionais\n",
    "        # observation = self.state  #by_frederic: retorna quando e um tipo Box\n",
    "        observation = {\n",
    "            \"time\": np.array([self.normaliza(self.time, self.lb_time, self.ub_time)]),\n",
    "            \"MT\": self.normaliza(self.MT, self.lb_MT, self.ub_MT),\n",
    "            \"EU\": self.normaliza(self.EU, self.lb_EU, self.ub_EU),\n",
    "            \"BA\": self.normaliza(self.BA, self.lb_BA, self.ub_BA),\n",
    "            \"IN\": self.normaliza(self.IN, self.lb_IN, self.ub_IN),\n",
    "            \"OU\": self.normaliza(self.OU, self.lb_OU, self.ub_OU),\n",
    "            \"state\": self.state,\n",
    "        }  # by_frederic: retorna quando e um tipo Dict\n",
    "\n",
    "        return (\n",
    "            observation,\n",
    "            reward,\n",
    "            done,\n",
    "            truncated,\n",
    "            info,\n",
    "        )  # , exprofit   # by_frederic:\n",
    "\n",
    "    ######################\n",
    "    #       reset        #\n",
    "    ######################\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        #super().reset(seed=None)\n",
    "        Somn.priorqpr = heapdict()\n",
    "        # Somn.priorqsu = heapdict()\n",
    "        # Somn.priorqva = heapdict()\n",
    "        self.MT = np.random.randint(0, self.MAXFT, self.M)\n",
    "        self.EU = np.random.random(self.M) * self.MAXEU\n",
    "        self.BA = np.random.randint(0, self.MAXFT, self.M)\n",
    "        self.IN = np.random.randint(0, self.MAXFT, self.M)\n",
    "        self.OU = np.random.randint(0, self.MAXFT, self.M)\n",
    "        Somn.time = 1\n",
    "        Demand.load = 1\n",
    "        Demand.reject = 0\n",
    "        Demand.reject_w_waste=0\n",
    "\n",
    "        self.YA = [Yard(self.Y, self.M, self.MAXFT) for _ in range(self.Y)]\n",
    "\n",
    "        arrayState = []\n",
    "        for i in range(self.N):\n",
    "            self.DE[i](Somn.time)\n",
    "            aux_row = [\n",
    "                self.normaliza(x=self.DE[i].ST, min=self.lb_ST, max=self.ub_ST),\n",
    "                # Somn.time,\n",
    "                # self.DE[i].SP,\n",
    "                self.normaliza(x=self.DE[i].LT, min=self.lb_LT, max=self.ub_LT),\n",
    "                # self.DE[i].VA,\n",
    "                # self.DE[i].SU,\n",
    "                # self.DE[i].PR,\n",
    "                self.normaliza(x=self.DE[i].DO, min=self.lb_DO, max=self.ub_DO),\n",
    "                self.normaliza(x=self.DE[i].TP, min=self.lb_TP, max=self.ub_TP),\n",
    "            ]\n",
    "            arrayState.append(aux_row)\n",
    "\n",
    "        self.state = np.array(arrayState)\n",
    "\n",
    "        info = dict()\n",
    "        # observation = (self.state, info)  # by_frederic: retorna quando o tipo Ã© Box\n",
    "        observation = {\n",
    "            \"time\": np.array([self.normaliza(self.time, self.lb_time, self.ub_time)]),\n",
    "            \"MT\": self.normaliza(self.MT, self.lb_MT, self.ub_MT),\n",
    "            \"EU\": self.normaliza(self.EU, self.lb_EU, self.ub_EU),\n",
    "            \"BA\": self.normaliza(self.BA, self.lb_BA, self.ub_BA),\n",
    "            \"IN\": self.normaliza(self.IN, self.lb_IN, self.ub_IN),\n",
    "            \"OU\": self.normaliza(self.OU, self.lb_OU, self.ub_OU),\n",
    "            \"state\": self.state,\n",
    "        }  # by_frederic: retorna quando e um tipo Dict\n",
    "\n",
    "        return (observation, info)  # by_frederic: para se adequar ao Gymnasium\n",
    "\n",
    "    ######################\n",
    "    #       render       #\n",
    "    ######################\n",
    "\n",
    "    def render(self):\n",
    "        # print(\"Current state (RENDER): \\n\", self.state)\n",
    "        pass\n",
    "\n",
    "    ######################\n",
    "    #       close        #\n",
    "    ######################\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "044f9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "def custom_make_env(atraso: int = None):\n",
    "    env = Somn(Y=10,M=5,N=7,MAXDO=10,MAXAM=5,MAXPR=1.5,MAXPE=10,MAXFT=5,MAXMT=3,MAXTI=2,MAXEU = 10, atraso=atraso)\n",
    "    env = Monitor(env)  # record stats such as returns\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e4170da",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    #'method': 'bayes'\n",
    "    'method': 'random'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "862b681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {\n",
    "    'name': 'mean_reward_test',\n",
    "    'goal': 'maximize'\n",
    "    }\n",
    "\n",
    "config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55763b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'batch_size': {\n",
    "        'values': [16, 32, 64, 128, 256]\n",
    "      },\n",
    "    # 'clip_range': {\n",
    "    #     'values': [0.1, 0.2, 0.3]\n",
    "    #     },\n",
    "    'ent_coef': {\n",
    "        'distribution': 'uniform',\n",
    "        'max': 0.01,\n",
    "        'min': 0\n",
    "    },\n",
    "    'gae_lambda': {\n",
    "        'distribution': 'uniform',\n",
    "        'max': 1,\n",
    "        'min': 0.9\n",
    "    },\n",
    "    'gamma': {\n",
    "        'distribution': 'uniform',\n",
    "        'max': 1,\n",
    "        'min': 0.8\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'max': 0.0009,\n",
    "        'min': 1e-05\n",
    "    },\n",
    "    # 'max_grad_norm': {\n",
    "    #     'distribution': 'uniform',\n",
    "    #     'max': 1,\n",
    "    #     'min': 0.5\n",
    "    # },\n",
    "    'n_epochs': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'max': 30,\n",
    "        'min': 5\n",
    "    },\n",
    "    'n_steps': {\n",
    "        'values': [1024, 1536, 2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840, 4096]\n",
    "    },\n",
    "    # 'stats_window_size': {\n",
    "    #     'distribution': 'int_uniform',\n",
    "    #     'max': 200,\n",
    "    #     'min': 50\n",
    "    # },\n",
    "    'target_kl': {\n",
    "        'distribution': 'uniform',\n",
    "        'max': 0.03,\n",
    "        'min': 0.003\n",
    "    },\n",
    "    # 'total_timesteps': {\n",
    "    #     'distribution': 'uniform',\n",
    "    #     'max': 2000000,\n",
    "    #     'min': 50000\n",
    "    # },\n",
    "    # 'vf_coef': {\n",
    "    #     'distribution': 'uniform',\n",
    "    #     'max': 1,\n",
    "    #     'min': 0.5\n",
    "    #}\n",
    "}\n",
    "\n",
    "config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bb22c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4bb3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(config, project=\"Sintonia_OPTSIMFLEX_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94bdaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = '0as0bgog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0344679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b89f9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "#import gym\n",
    "\n",
    "#from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "\n",
    "#from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "atraso = -1\n",
    "\n",
    "def make_env():\n",
    "    env = Somn(Y=10,M=10,N=10,MAXDO=10,MAXAM=3,MAXPR=2,MAXPE=10,MAXFT=5,MAXMT=3,MAXTI=2,MAXEU = 5, atraso=atraso)\n",
    "    #env = Somn(Y=10,M=5,N=7,MAXDO=10,MAXAM=5,MAXPR=1.5,MAXPE=10,MAXFT=5,MAXMT=3,MAXTI=2,MAXEU = 10)\n",
    "    env = Monitor(env)  # record stats such as returns\n",
    "    return env\n",
    "\n",
    "def treinamento1(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "        wandb.tensorboard.unpatch()\n",
    "    #wandb.tensorboard.patch(root_logdir=\"/content/drive/MyDrive/OPTSIMFLEX_1/runs\")\n",
    "    wandb.tensorboard.patch(root_logdir=\"./runs\")\n",
    "    run1 = wandb.init(project='Sintonia_OPTSIMFLEX_01', config=config, save_code=True)\n",
    "    #project=\"Somn_01\",\n",
    "    #config=config,\n",
    "    #sync_tensorboard=False,  # auto-upload sb3's tensorboard metrics\n",
    "    #monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    #save_code=True,  # optional\n",
    "    #) as run:\n",
    "    # settings=wandb.Settings(start_method=\"thread\")\n",
    "    # ) as run:\n",
    "\n",
    "    # If called by wandb.agent, as below,\n",
    "    # this config will be set by Sweep Controller\n",
    "    config = wandb.config\n",
    "    env1 = DummyVecEnv([lambda: custom_make_env(atraso)])\n",
    "\n",
    "\n",
    "    # algoritmos = [PPO, RecurrentPPO]\n",
    "    # politica ={PPO: \"MultiInputPolicy\", RecurrentPPO: \"MlpLstmPolicy\"}\n",
    "\n",
    "\n",
    "    model1 = PPO(\n",
    "        policy=\"MultiInputPolicy\",\n",
    "        env=env1,\n",
    "        learning_rate=config.learning_rate,\n",
    "        n_steps=config.n_steps,\n",
    "        batch_size=config.batch_size,\n",
    "        n_epochs=config.n_epochs,\n",
    "        gamma=config.gamma,\n",
    "        gae_lambda=config.gae_lambda,\n",
    "        #clip_range=config.clip_range,\n",
    "        ent_coef=config.ent_coef,\n",
    "        #vf_coef=config.vf_coef,\n",
    "        #max_grad_norm=config.max_grad_norm,\n",
    "        target_kl=config.target_kl,\n",
    "        #stats_window_size=config.stats_window_size,\n",
    "        verbose=0,\n",
    "        #seed = 2023,\n",
    "        device='cpu',\n",
    "        #tensorboard_log=f\"/content/drive/MyDrive/SOMN2/runs/{run1.id}\"\n",
    "        tensorboard_log=f\"runs/{run1.id}\"\n",
    "    )\n",
    "\n",
    "    model1.learn(total_timesteps=1000000)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8fc718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "#import gym\n",
    "\n",
    "#from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "\n",
    "#from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# def make_env():\n",
    "#     env = Somn(Y=10,M=5,N=7,MAXDO=10,MAXAM=5,MAXPR=1.5,MAXPE=10,MAXFT=5,MAXMT=3,MAXTI=2,MAXEU = 10)\n",
    "#     env = Monitor(env)  # record stats such as returns\n",
    "#     return env\n",
    "\n",
    "def treinamento2(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "        wandb.tensorboard.unpatch()\n",
    "    #wandb.tensorboard.patch(root_logdir=\"/content/drive/MyDrive/OPTSIMFLEX_1/runs\")\n",
    "    wandb.tensorboard.patch(root_logdir=\"./runs\")\n",
    "    run2 = wandb.init(project='Somn_02', config=config, save_code=True)\n",
    "    #project=\"Somn_01\",\n",
    "    #config=config,\n",
    "    #sync_tensorboard=False,  # auto-upload sb3's tensorboard metrics\n",
    "    #monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    #save_code=True,  # optional\n",
    "    #) as run:\n",
    "    # settings=wandb.Settings(start_method=\"thread\")\n",
    "    # ) as run:\n",
    "\n",
    "    # If called by wandb.agent, as below,\n",
    "    # this config will be set by Sweep Controller\n",
    "    config = wandb.config\n",
    "    env2 = DummyVecEnv([lambda: custom_make_env(atraso)])\n",
    "\n",
    "\n",
    "    # algoritmos = [PPO, RecurrentPPO]\n",
    "    # politica ={PPO: \"MultiInputPolicy\", RecurrentPPO: \"MlpLstmPolicy\"}\n",
    "\n",
    "\n",
    "\n",
    "    model2 = RecurrentPPO(\n",
    "        policy=\"MultiInputLstmPolicy\",\n",
    "        env=env2,\n",
    "        learning_rate=config.learning_rate,\n",
    "        n_steps=config.n_steps,\n",
    "        batch_size=config.batch_size,\n",
    "        n_epochs=config.n_epochs,\n",
    "        gamma=config.gamma,\n",
    "        gae_lambda=config.gae_lambda,\n",
    "        #clip_range=config.clip_range,\n",
    "        ent_coef=config.ent_coef,\n",
    "        #vf_coef=config.vf_coef,\n",
    "        #max_grad_norm=config.max_grad_norm,\n",
    "        target_kl=config.target_kl,\n",
    "        #stats_window_size=config.stats_window_size,\n",
    "        verbose=0,\n",
    "        #seed = 2023,\n",
    "        device='cuda',\n",
    "        #tensorboard_log=f\"/content/drive/MyDrive/SOMN2/runs/{run2.id}\"\n",
    "        tensorboard_log=f\"runs/{run2.id}\"\n",
    "    )\n",
    "\n",
    "    model2.learn(total_timesteps=1000000)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9aa2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    #th.manual_seed(seed)\n",
    "    #th.cuda.manual_seed(seed)\n",
    "    #th.backends.cudnn.deterministic = True\n",
    "    #env.seed(seed)\n",
    "\n",
    "##One call at beginning is enough\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfbdda5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4012ae7473d742f0b7b8bd9914b2461a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01128888888876342, max=1.0)â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\notebook-heitor\\Documents\\GitHub\\SOMN\\wandb\\run-20231006_021525-w48i9df9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01/runs/w48i9df9' target=\"_blank\">glamorous-sweep-18</a></strong> to <a href='https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01/sweeps/0as0bgog' target=\"_blank\">https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01/sweeps/0as0bgog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01' target=\"_blank\">https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01/sweeps/0as0bgog' target=\"_blank\">https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01/sweeps/0as0bgog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01/runs/w48i9df9' target=\"_blank\">https://wandb.ai/lacmor/Sintonia_OPTSIMFLEX_01/runs/w48i9df9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wandb.agent(sweep_id, treinamento2, count=20)\n",
    "wandb.agent(sweep_id, treinamento1, count=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
